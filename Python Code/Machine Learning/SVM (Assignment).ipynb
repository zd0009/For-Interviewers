{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSCA 31009 Machine Learning & Predictive Analytics\n",
    "## Assignment 5\n",
    "## Duo Zhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "f94-pvigXOyA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics as skm\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x3yHvUQ9s-O"
   },
   "source": [
    "## 1. Data Processing:\n",
    "\n",
    "a) Import the data: You are provided separate .csv files for train and test.\n",
    "\n",
    "Train shape: (507, 148)\n",
    "Test shape: (168, 148)\n",
    "b) Remove any rows that have missing data across both sets of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSHLpFR_sr8Y",
    "outputId": "44c83005-6144-492a-92f3-24dc72a62a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(507, 148)\n",
      "(168, 148)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "\n",
    "train = train.dropna(how='any', axis = 0)\n",
    "test = test.dropna(how='any', axis = 0)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "JOIuPQAtObSW",
    "outputId": "b01c7c8d-f9ff-41d9-fc15-a01edaf4ba76"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>BrdIndx</th>\n",
       "      <th>Area</th>\n",
       "      <th>Round</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Compact</th>\n",
       "      <th>ShpIndx</th>\n",
       "      <th>Mean_G</th>\n",
       "      <th>Mean_R</th>\n",
       "      <th>Mean_NIR</th>\n",
       "      <th>SD_G</th>\n",
       "      <th>SD_R</th>\n",
       "      <th>SD_NIR</th>\n",
       "      <th>LW</th>\n",
       "      <th>GLCM1</th>\n",
       "      <th>Rect</th>\n",
       "      <th>GLCM2</th>\n",
       "      <th>Dens</th>\n",
       "      <th>Assym</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>BordLngth</th>\n",
       "      <th>GLCM3</th>\n",
       "      <th>BrdIndx_40</th>\n",
       "      <th>Area_40</th>\n",
       "      <th>Round_40</th>\n",
       "      <th>Bright_40</th>\n",
       "      <th>Compact_40</th>\n",
       "      <th>ShpIndx_40</th>\n",
       "      <th>Mean_G_40</th>\n",
       "      <th>Mean_R_40</th>\n",
       "      <th>Mean_NIR_40</th>\n",
       "      <th>SD_G_40</th>\n",
       "      <th>SD_R_40</th>\n",
       "      <th>SD_NIR_40</th>\n",
       "      <th>LW_40</th>\n",
       "      <th>GLCM1_40</th>\n",
       "      <th>Rect_40</th>\n",
       "      <th>GLCM2_40</th>\n",
       "      <th>Dens_40</th>\n",
       "      <th>Assym_40</th>\n",
       "      <th>...</th>\n",
       "      <th>Round_120</th>\n",
       "      <th>Bright_120</th>\n",
       "      <th>Compact_120</th>\n",
       "      <th>ShpIndx_120</th>\n",
       "      <th>Mean_G_120</th>\n",
       "      <th>Mean_R_120</th>\n",
       "      <th>Mean_NIR_120</th>\n",
       "      <th>SD_G_120</th>\n",
       "      <th>SD_R_120</th>\n",
       "      <th>SD_NIR_120</th>\n",
       "      <th>LW_120</th>\n",
       "      <th>GLCM1_120</th>\n",
       "      <th>Rect_120</th>\n",
       "      <th>GLCM2_120</th>\n",
       "      <th>Dens_120</th>\n",
       "      <th>Assym_120</th>\n",
       "      <th>NDVI_120</th>\n",
       "      <th>BordLngth_120</th>\n",
       "      <th>GLCM3_120</th>\n",
       "      <th>BrdIndx_140</th>\n",
       "      <th>Area_140</th>\n",
       "      <th>Round_140</th>\n",
       "      <th>Bright_140</th>\n",
       "      <th>Compact_140</th>\n",
       "      <th>ShpIndx_140</th>\n",
       "      <th>Mean_G_140</th>\n",
       "      <th>Mean_R_140</th>\n",
       "      <th>Mean_NIR_140</th>\n",
       "      <th>SD_G_140</th>\n",
       "      <th>SD_R_140</th>\n",
       "      <th>SD_NIR_140</th>\n",
       "      <th>LW_140</th>\n",
       "      <th>GLCM1_140</th>\n",
       "      <th>Rect_140</th>\n",
       "      <th>GLCM2_140</th>\n",
       "      <th>Dens_140</th>\n",
       "      <th>Assym_140</th>\n",
       "      <th>NDVI_140</th>\n",
       "      <th>BordLngth_140</th>\n",
       "      <th>GLCM3_140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>concrete</td>\n",
       "      <td>1.32</td>\n",
       "      <td>131</td>\n",
       "      <td>0.81</td>\n",
       "      <td>222.74</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "      <td>192.94</td>\n",
       "      <td>235.11</td>\n",
       "      <td>240.15</td>\n",
       "      <td>11.24</td>\n",
       "      <td>11.47</td>\n",
       "      <td>11.24</td>\n",
       "      <td>8.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>4322.47</td>\n",
       "      <td>1.32</td>\n",
       "      <td>131</td>\n",
       "      <td>0.81</td>\n",
       "      <td>222.74</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "      <td>192.94</td>\n",
       "      <td>235.11</td>\n",
       "      <td>240.15</td>\n",
       "      <td>11.24</td>\n",
       "      <td>11.47</td>\n",
       "      <td>11.24</td>\n",
       "      <td>8.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.43</td>\n",
       "      <td>189.73</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.40</td>\n",
       "      <td>163.61</td>\n",
       "      <td>201.29</td>\n",
       "      <td>204.28</td>\n",
       "      <td>25.25</td>\n",
       "      <td>28.33</td>\n",
       "      <td>28.30</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.35</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1188</td>\n",
       "      <td>1298.99</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9172</td>\n",
       "      <td>2.50</td>\n",
       "      <td>185.14</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.95</td>\n",
       "      <td>159.45</td>\n",
       "      <td>196.43</td>\n",
       "      <td>199.53</td>\n",
       "      <td>27.81</td>\n",
       "      <td>31.55</td>\n",
       "      <td>31.15</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1512</td>\n",
       "      <td>1287.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shadow</td>\n",
       "      <td>1.59</td>\n",
       "      <td>864</td>\n",
       "      <td>0.94</td>\n",
       "      <td>47.56</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.87</td>\n",
       "      <td>36.82</td>\n",
       "      <td>48.78</td>\n",
       "      <td>57.09</td>\n",
       "      <td>8.15</td>\n",
       "      <td>8.02</td>\n",
       "      <td>8.36</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.85</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>220</td>\n",
       "      <td>3331.33</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1353</td>\n",
       "      <td>0.46</td>\n",
       "      <td>49.12</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.33</td>\n",
       "      <td>38.29</td>\n",
       "      <td>50.40</td>\n",
       "      <td>58.67</td>\n",
       "      <td>8.84</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6.96</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46</td>\n",
       "      <td>49.82</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>38.88</td>\n",
       "      <td>51.21</td>\n",
       "      <td>59.38</td>\n",
       "      <td>10.07</td>\n",
       "      <td>11.89</td>\n",
       "      <td>12.01</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.96</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>196</td>\n",
       "      <td>2659.74</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1377</td>\n",
       "      <td>0.46</td>\n",
       "      <td>49.82</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>38.88</td>\n",
       "      <td>51.21</td>\n",
       "      <td>59.38</td>\n",
       "      <td>10.07</td>\n",
       "      <td>11.89</td>\n",
       "      <td>12.01</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.96</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>196</td>\n",
       "      <td>2659.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shadow</td>\n",
       "      <td>1.41</td>\n",
       "      <td>409</td>\n",
       "      <td>1.00</td>\n",
       "      <td>51.38</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.53</td>\n",
       "      <td>41.72</td>\n",
       "      <td>51.96</td>\n",
       "      <td>60.48</td>\n",
       "      <td>8.11</td>\n",
       "      <td>9.20</td>\n",
       "      <td>9.61</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>124</td>\n",
       "      <td>2816.16</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1094</td>\n",
       "      <td>1.91</td>\n",
       "      <td>49.05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.98</td>\n",
       "      <td>40.06</td>\n",
       "      <td>50.07</td>\n",
       "      <td>57.02</td>\n",
       "      <td>10.54</td>\n",
       "      <td>10.81</td>\n",
       "      <td>11.55</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>46.57</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.79</td>\n",
       "      <td>40.01</td>\n",
       "      <td>46.34</td>\n",
       "      <td>53.36</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.78</td>\n",
       "      <td>12.31</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>602</td>\n",
       "      <td>1432.44</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5932</td>\n",
       "      <td>1.69</td>\n",
       "      <td>55.06</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.89</td>\n",
       "      <td>59.76</td>\n",
       "      <td>48.66</td>\n",
       "      <td>56.76</td>\n",
       "      <td>30.65</td>\n",
       "      <td>18.59</td>\n",
       "      <td>18.75</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.63</td>\n",
       "      <td>8.32</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1198</td>\n",
       "      <td>720.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tree</td>\n",
       "      <td>2.58</td>\n",
       "      <td>187</td>\n",
       "      <td>1.91</td>\n",
       "      <td>70.08</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.11</td>\n",
       "      <td>93.13</td>\n",
       "      <td>55.20</td>\n",
       "      <td>61.92</td>\n",
       "      <td>28.60</td>\n",
       "      <td>15.88</td>\n",
       "      <td>15.09</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.50</td>\n",
       "      <td>7.28</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.26</td>\n",
       "      <td>170</td>\n",
       "      <td>2442.01</td>\n",
       "      <td>2.86</td>\n",
       "      <td>871</td>\n",
       "      <td>2.22</td>\n",
       "      <td>90.21</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.93</td>\n",
       "      <td>117.16</td>\n",
       "      <td>72.76</td>\n",
       "      <td>80.70</td>\n",
       "      <td>42.07</td>\n",
       "      <td>27.05</td>\n",
       "      <td>27.38</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.51</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>1.65</td>\n",
       "      <td>69.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.22</td>\n",
       "      <td>86.22</td>\n",
       "      <td>58.03</td>\n",
       "      <td>65.15</td>\n",
       "      <td>47.93</td>\n",
       "      <td>26.70</td>\n",
       "      <td>27.67</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.56</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>524</td>\n",
       "      <td>891.36</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1660</td>\n",
       "      <td>1.65</td>\n",
       "      <td>69.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.22</td>\n",
       "      <td>86.22</td>\n",
       "      <td>58.03</td>\n",
       "      <td>65.15</td>\n",
       "      <td>47.93</td>\n",
       "      <td>26.70</td>\n",
       "      <td>27.67</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.56</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>524</td>\n",
       "      <td>891.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asphalt</td>\n",
       "      <td>2.60</td>\n",
       "      <td>116</td>\n",
       "      <td>2.05</td>\n",
       "      <td>89.57</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.02</td>\n",
       "      <td>73.17</td>\n",
       "      <td>94.89</td>\n",
       "      <td>100.64</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.27</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.59</td>\n",
       "      <td>6.02</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>130</td>\n",
       "      <td>4912.28</td>\n",
       "      <td>4.02</td>\n",
       "      <td>501</td>\n",
       "      <td>2.33</td>\n",
       "      <td>100.69</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.40</td>\n",
       "      <td>83.66</td>\n",
       "      <td>106.21</td>\n",
       "      <td>112.19</td>\n",
       "      <td>15.39</td>\n",
       "      <td>16.50</td>\n",
       "      <td>16.44</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.28</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.79</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>96.24</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>82.84</td>\n",
       "      <td>100.31</td>\n",
       "      <td>105.55</td>\n",
       "      <td>30.51</td>\n",
       "      <td>32.60</td>\n",
       "      <td>32.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.62</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>496</td>\n",
       "      <td>1194.76</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2351</td>\n",
       "      <td>1.05</td>\n",
       "      <td>96.24</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>82.84</td>\n",
       "      <td>100.31</td>\n",
       "      <td>105.55</td>\n",
       "      <td>30.51</td>\n",
       "      <td>32.60</td>\n",
       "      <td>32.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.62</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>496</td>\n",
       "      <td>1194.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class  BrdIndx  Area  ...  NDVI_140  BordLngth_140  GLCM3_140\n",
       "0  concrete      1.32   131  ...     -0.10           1512    1287.52\n",
       "1    shadow      1.59   864  ...     -0.14            196    2659.74\n",
       "2    shadow      1.41   409  ...      0.10           1198     720.38\n",
       "3      tree      2.58   187  ...      0.20            524     891.36\n",
       "4   asphalt      2.60   116  ...     -0.10            496    1194.76\n",
       "\n",
       "[5 rows x 148 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43ayRouCNOwZ"
   },
   "source": [
    "c) The target variable (dependent variable) is called \"class\", make sure to separate this out into a \"y_train\" and \"y_test\" and remove from your \"X_train\" and \"X_test\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2uAfU2d-NTaZ"
   },
   "outputs": [],
   "source": [
    "y_train = train['class']\n",
    "y_test  = test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9vqhsZMO9op",
    "outputId": "db85266f-8d15-4c4a-e569-6a15b41a5cd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    concrete \n",
       "1      shadow \n",
       "2      shadow \n",
       "3        tree \n",
       "4     asphalt \n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "18exh6ezPCtG",
    "outputId": "de0f2ca1-0ba4-4114-a5fe-7fe666687de0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrdIndx</th>\n",
       "      <th>Area</th>\n",
       "      <th>Round</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Compact</th>\n",
       "      <th>ShpIndx</th>\n",
       "      <th>Mean_G</th>\n",
       "      <th>Mean_R</th>\n",
       "      <th>Mean_NIR</th>\n",
       "      <th>SD_G</th>\n",
       "      <th>SD_R</th>\n",
       "      <th>SD_NIR</th>\n",
       "      <th>LW</th>\n",
       "      <th>GLCM1</th>\n",
       "      <th>Rect</th>\n",
       "      <th>GLCM2</th>\n",
       "      <th>Dens</th>\n",
       "      <th>Assym</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>BordLngth</th>\n",
       "      <th>GLCM3</th>\n",
       "      <th>BrdIndx_40</th>\n",
       "      <th>Area_40</th>\n",
       "      <th>Round_40</th>\n",
       "      <th>Bright_40</th>\n",
       "      <th>Compact_40</th>\n",
       "      <th>ShpIndx_40</th>\n",
       "      <th>Mean_G_40</th>\n",
       "      <th>Mean_R_40</th>\n",
       "      <th>Mean_NIR_40</th>\n",
       "      <th>SD_G_40</th>\n",
       "      <th>SD_R_40</th>\n",
       "      <th>SD_NIR_40</th>\n",
       "      <th>LW_40</th>\n",
       "      <th>GLCM1_40</th>\n",
       "      <th>Rect_40</th>\n",
       "      <th>GLCM2_40</th>\n",
       "      <th>Dens_40</th>\n",
       "      <th>Assym_40</th>\n",
       "      <th>NDVI_40</th>\n",
       "      <th>...</th>\n",
       "      <th>Round_120</th>\n",
       "      <th>Bright_120</th>\n",
       "      <th>Compact_120</th>\n",
       "      <th>ShpIndx_120</th>\n",
       "      <th>Mean_G_120</th>\n",
       "      <th>Mean_R_120</th>\n",
       "      <th>Mean_NIR_120</th>\n",
       "      <th>SD_G_120</th>\n",
       "      <th>SD_R_120</th>\n",
       "      <th>SD_NIR_120</th>\n",
       "      <th>LW_120</th>\n",
       "      <th>GLCM1_120</th>\n",
       "      <th>Rect_120</th>\n",
       "      <th>GLCM2_120</th>\n",
       "      <th>Dens_120</th>\n",
       "      <th>Assym_120</th>\n",
       "      <th>NDVI_120</th>\n",
       "      <th>BordLngth_120</th>\n",
       "      <th>GLCM3_120</th>\n",
       "      <th>BrdIndx_140</th>\n",
       "      <th>Area_140</th>\n",
       "      <th>Round_140</th>\n",
       "      <th>Bright_140</th>\n",
       "      <th>Compact_140</th>\n",
       "      <th>ShpIndx_140</th>\n",
       "      <th>Mean_G_140</th>\n",
       "      <th>Mean_R_140</th>\n",
       "      <th>Mean_NIR_140</th>\n",
       "      <th>SD_G_140</th>\n",
       "      <th>SD_R_140</th>\n",
       "      <th>SD_NIR_140</th>\n",
       "      <th>LW_140</th>\n",
       "      <th>GLCM1_140</th>\n",
       "      <th>Rect_140</th>\n",
       "      <th>GLCM2_140</th>\n",
       "      <th>Dens_140</th>\n",
       "      <th>Assym_140</th>\n",
       "      <th>NDVI_140</th>\n",
       "      <th>BordLngth_140</th>\n",
       "      <th>GLCM3_140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.32</td>\n",
       "      <td>131</td>\n",
       "      <td>0.81</td>\n",
       "      <td>222.74</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "      <td>192.94</td>\n",
       "      <td>235.11</td>\n",
       "      <td>240.15</td>\n",
       "      <td>11.24</td>\n",
       "      <td>11.47</td>\n",
       "      <td>11.24</td>\n",
       "      <td>8.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>4322.47</td>\n",
       "      <td>1.32</td>\n",
       "      <td>131</td>\n",
       "      <td>0.81</td>\n",
       "      <td>222.74</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "      <td>192.94</td>\n",
       "      <td>235.11</td>\n",
       "      <td>240.15</td>\n",
       "      <td>11.24</td>\n",
       "      <td>11.47</td>\n",
       "      <td>11.24</td>\n",
       "      <td>8.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.43</td>\n",
       "      <td>189.73</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.40</td>\n",
       "      <td>163.61</td>\n",
       "      <td>201.29</td>\n",
       "      <td>204.28</td>\n",
       "      <td>25.25</td>\n",
       "      <td>28.33</td>\n",
       "      <td>28.30</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.35</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1188</td>\n",
       "      <td>1298.99</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9172</td>\n",
       "      <td>2.50</td>\n",
       "      <td>185.14</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.95</td>\n",
       "      <td>159.45</td>\n",
       "      <td>196.43</td>\n",
       "      <td>199.53</td>\n",
       "      <td>27.81</td>\n",
       "      <td>31.55</td>\n",
       "      <td>31.15</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1512</td>\n",
       "      <td>1287.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.59</td>\n",
       "      <td>864</td>\n",
       "      <td>0.94</td>\n",
       "      <td>47.56</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.87</td>\n",
       "      <td>36.82</td>\n",
       "      <td>48.78</td>\n",
       "      <td>57.09</td>\n",
       "      <td>8.15</td>\n",
       "      <td>8.02</td>\n",
       "      <td>8.36</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.85</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>220</td>\n",
       "      <td>3331.33</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1353</td>\n",
       "      <td>0.46</td>\n",
       "      <td>49.12</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.33</td>\n",
       "      <td>38.29</td>\n",
       "      <td>50.40</td>\n",
       "      <td>58.67</td>\n",
       "      <td>8.84</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6.96</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46</td>\n",
       "      <td>49.82</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>38.88</td>\n",
       "      <td>51.21</td>\n",
       "      <td>59.38</td>\n",
       "      <td>10.07</td>\n",
       "      <td>11.89</td>\n",
       "      <td>12.01</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.96</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>196</td>\n",
       "      <td>2659.74</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1377</td>\n",
       "      <td>0.46</td>\n",
       "      <td>49.82</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>38.88</td>\n",
       "      <td>51.21</td>\n",
       "      <td>59.38</td>\n",
       "      <td>10.07</td>\n",
       "      <td>11.89</td>\n",
       "      <td>12.01</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.96</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>196</td>\n",
       "      <td>2659.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.41</td>\n",
       "      <td>409</td>\n",
       "      <td>1.00</td>\n",
       "      <td>51.38</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.53</td>\n",
       "      <td>41.72</td>\n",
       "      <td>51.96</td>\n",
       "      <td>60.48</td>\n",
       "      <td>8.11</td>\n",
       "      <td>9.20</td>\n",
       "      <td>9.61</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>124</td>\n",
       "      <td>2816.16</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1094</td>\n",
       "      <td>1.91</td>\n",
       "      <td>49.05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.98</td>\n",
       "      <td>40.06</td>\n",
       "      <td>50.07</td>\n",
       "      <td>57.02</td>\n",
       "      <td>10.54</td>\n",
       "      <td>10.81</td>\n",
       "      <td>11.55</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>46.57</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.79</td>\n",
       "      <td>40.01</td>\n",
       "      <td>46.34</td>\n",
       "      <td>53.36</td>\n",
       "      <td>14.49</td>\n",
       "      <td>11.78</td>\n",
       "      <td>12.31</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>602</td>\n",
       "      <td>1432.44</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5932</td>\n",
       "      <td>1.69</td>\n",
       "      <td>55.06</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.89</td>\n",
       "      <td>59.76</td>\n",
       "      <td>48.66</td>\n",
       "      <td>56.76</td>\n",
       "      <td>30.65</td>\n",
       "      <td>18.59</td>\n",
       "      <td>18.75</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.63</td>\n",
       "      <td>8.32</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1198</td>\n",
       "      <td>720.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.58</td>\n",
       "      <td>187</td>\n",
       "      <td>1.91</td>\n",
       "      <td>70.08</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.11</td>\n",
       "      <td>93.13</td>\n",
       "      <td>55.20</td>\n",
       "      <td>61.92</td>\n",
       "      <td>28.60</td>\n",
       "      <td>15.88</td>\n",
       "      <td>15.09</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.50</td>\n",
       "      <td>7.28</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.26</td>\n",
       "      <td>170</td>\n",
       "      <td>2442.01</td>\n",
       "      <td>2.86</td>\n",
       "      <td>871</td>\n",
       "      <td>2.22</td>\n",
       "      <td>90.21</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.93</td>\n",
       "      <td>117.16</td>\n",
       "      <td>72.76</td>\n",
       "      <td>80.70</td>\n",
       "      <td>42.07</td>\n",
       "      <td>27.05</td>\n",
       "      <td>27.38</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.51</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>1.65</td>\n",
       "      <td>69.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.22</td>\n",
       "      <td>86.22</td>\n",
       "      <td>58.03</td>\n",
       "      <td>65.15</td>\n",
       "      <td>47.93</td>\n",
       "      <td>26.70</td>\n",
       "      <td>27.67</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.56</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>524</td>\n",
       "      <td>891.36</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1660</td>\n",
       "      <td>1.65</td>\n",
       "      <td>69.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.22</td>\n",
       "      <td>86.22</td>\n",
       "      <td>58.03</td>\n",
       "      <td>65.15</td>\n",
       "      <td>47.93</td>\n",
       "      <td>26.70</td>\n",
       "      <td>27.67</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.56</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>524</td>\n",
       "      <td>891.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.60</td>\n",
       "      <td>116</td>\n",
       "      <td>2.05</td>\n",
       "      <td>89.57</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.02</td>\n",
       "      <td>73.17</td>\n",
       "      <td>94.89</td>\n",
       "      <td>100.64</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.27</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.59</td>\n",
       "      <td>6.02</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>130</td>\n",
       "      <td>4912.28</td>\n",
       "      <td>4.02</td>\n",
       "      <td>501</td>\n",
       "      <td>2.33</td>\n",
       "      <td>100.69</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.40</td>\n",
       "      <td>83.66</td>\n",
       "      <td>106.21</td>\n",
       "      <td>112.19</td>\n",
       "      <td>15.39</td>\n",
       "      <td>16.50</td>\n",
       "      <td>16.44</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.28</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>96.24</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>82.84</td>\n",
       "      <td>100.31</td>\n",
       "      <td>105.55</td>\n",
       "      <td>30.51</td>\n",
       "      <td>32.60</td>\n",
       "      <td>32.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.62</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>496</td>\n",
       "      <td>1194.76</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2351</td>\n",
       "      <td>1.05</td>\n",
       "      <td>96.24</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>82.84</td>\n",
       "      <td>100.31</td>\n",
       "      <td>105.55</td>\n",
       "      <td>30.51</td>\n",
       "      <td>32.60</td>\n",
       "      <td>32.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.62</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>496</td>\n",
       "      <td>1194.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BrdIndx  Area  Round  Bright  ...  Assym_140  NDVI_140  BordLngth_140  GLCM3_140\n",
       "0     1.32   131   0.81  222.74  ...       0.98     -0.10           1512    1287.52\n",
       "1     1.59   864   0.94   47.56  ...       0.86     -0.14            196    2659.74\n",
       "2     1.41   409   1.00   51.38  ...       0.84      0.10           1198     720.38\n",
       "3     2.58   187   1.91   70.08  ...       0.96      0.20            524     891.36\n",
       "4     2.60   116   2.05   89.57  ...       0.08     -0.10            496    1194.76\n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(columns='class', inplace=True)\n",
    "test.drop(columns='class', inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NRg4YWlNkPv"
   },
   "source": [
    "d) Scale all features / predictors (NOT THE TARGET VARIABLE)\n",
    "\n",
    "Feel free to use the sklearn tool \"StandardScaler\" - more info here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html (Links to an external site.)\n",
    "\n",
    "Note: We need to scale here due to SVM. Please refer to previous assignments if you have forgotten appropriate scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7D4AmnV1Nysk"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Note we use train mean and sd to scale bith train and test data\n",
    "scaler.fit(train)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(train), columns=train.columns)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "o69ARAhVP5Xq",
    "outputId": "68848e01-f097-4fd3-bff8-8a6d9366d569"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrdIndx</th>\n",
       "      <th>Area</th>\n",
       "      <th>Round</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Compact</th>\n",
       "      <th>ShpIndx</th>\n",
       "      <th>Mean_G</th>\n",
       "      <th>Mean_R</th>\n",
       "      <th>Mean_NIR</th>\n",
       "      <th>SD_G</th>\n",
       "      <th>SD_R</th>\n",
       "      <th>SD_NIR</th>\n",
       "      <th>LW</th>\n",
       "      <th>GLCM1</th>\n",
       "      <th>Rect</th>\n",
       "      <th>GLCM2</th>\n",
       "      <th>Dens</th>\n",
       "      <th>Assym</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>BordLngth</th>\n",
       "      <th>GLCM3</th>\n",
       "      <th>BrdIndx_40</th>\n",
       "      <th>Area_40</th>\n",
       "      <th>Round_40</th>\n",
       "      <th>Bright_40</th>\n",
       "      <th>Compact_40</th>\n",
       "      <th>ShpIndx_40</th>\n",
       "      <th>Mean_G_40</th>\n",
       "      <th>Mean_R_40</th>\n",
       "      <th>Mean_NIR_40</th>\n",
       "      <th>SD_G_40</th>\n",
       "      <th>SD_R_40</th>\n",
       "      <th>SD_NIR_40</th>\n",
       "      <th>LW_40</th>\n",
       "      <th>GLCM1_40</th>\n",
       "      <th>Rect_40</th>\n",
       "      <th>GLCM2_40</th>\n",
       "      <th>Dens_40</th>\n",
       "      <th>Assym_40</th>\n",
       "      <th>NDVI_40</th>\n",
       "      <th>...</th>\n",
       "      <th>Round_120</th>\n",
       "      <th>Bright_120</th>\n",
       "      <th>Compact_120</th>\n",
       "      <th>ShpIndx_120</th>\n",
       "      <th>Mean_G_120</th>\n",
       "      <th>Mean_R_120</th>\n",
       "      <th>Mean_NIR_120</th>\n",
       "      <th>SD_G_120</th>\n",
       "      <th>SD_R_120</th>\n",
       "      <th>SD_NIR_120</th>\n",
       "      <th>LW_120</th>\n",
       "      <th>GLCM1_120</th>\n",
       "      <th>Rect_120</th>\n",
       "      <th>GLCM2_120</th>\n",
       "      <th>Dens_120</th>\n",
       "      <th>Assym_120</th>\n",
       "      <th>NDVI_120</th>\n",
       "      <th>BordLngth_120</th>\n",
       "      <th>GLCM3_120</th>\n",
       "      <th>BrdIndx_140</th>\n",
       "      <th>Area_140</th>\n",
       "      <th>Round_140</th>\n",
       "      <th>Bright_140</th>\n",
       "      <th>Compact_140</th>\n",
       "      <th>ShpIndx_140</th>\n",
       "      <th>Mean_G_140</th>\n",
       "      <th>Mean_R_140</th>\n",
       "      <th>Mean_NIR_140</th>\n",
       "      <th>SD_G_140</th>\n",
       "      <th>SD_R_140</th>\n",
       "      <th>SD_NIR_140</th>\n",
       "      <th>LW_140</th>\n",
       "      <th>GLCM1_140</th>\n",
       "      <th>Rect_140</th>\n",
       "      <th>GLCM2_140</th>\n",
       "      <th>Dens_140</th>\n",
       "      <th>Assym_140</th>\n",
       "      <th>NDVI_140</th>\n",
       "      <th>BordLngth_140</th>\n",
       "      <th>GLCM3_140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.221577</td>\n",
       "      <td>-0.675542</td>\n",
       "      <td>-0.476591</td>\n",
       "      <td>1.041138</td>\n",
       "      <td>-0.912836</td>\n",
       "      <td>-1.124818</td>\n",
       "      <td>0.703688</td>\n",
       "      <td>1.082655</td>\n",
       "      <td>1.094633</td>\n",
       "      <td>2.152514</td>\n",
       "      <td>2.642830</td>\n",
       "      <td>2.319919</td>\n",
       "      <td>-0.085216</td>\n",
       "      <td>-0.562228</td>\n",
       "      <td>1.006482</td>\n",
       "      <td>-0.780311</td>\n",
       "      <td>0.034290</td>\n",
       "      <td>0.483397</td>\n",
       "      <td>-0.604088</td>\n",
       "      <td>-1.224677</td>\n",
       "      <td>1.440512</td>\n",
       "      <td>-1.152568</td>\n",
       "      <td>-0.736193</td>\n",
       "      <td>-0.418816</td>\n",
       "      <td>1.005655</td>\n",
       "      <td>-0.945329</td>\n",
       "      <td>-1.191577</td>\n",
       "      <td>0.670774</td>\n",
       "      <td>1.049783</td>\n",
       "      <td>1.070957</td>\n",
       "      <td>1.539796</td>\n",
       "      <td>1.925406</td>\n",
       "      <td>1.753955</td>\n",
       "      <td>-0.202586</td>\n",
       "      <td>-1.229994</td>\n",
       "      <td>0.860960</td>\n",
       "      <td>-1.491551</td>\n",
       "      <td>0.240285</td>\n",
       "      <td>0.222749</td>\n",
       "      <td>-0.631085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.855411</td>\n",
       "      <td>1.102299</td>\n",
       "      <td>-0.926964</td>\n",
       "      <td>-1.439868</td>\n",
       "      <td>0.779929</td>\n",
       "      <td>1.144338</td>\n",
       "      <td>1.178201</td>\n",
       "      <td>0.115424</td>\n",
       "      <td>0.379037</td>\n",
       "      <td>0.282297</td>\n",
       "      <td>-0.201660</td>\n",
       "      <td>-2.788451</td>\n",
       "      <td>1.202081</td>\n",
       "      <td>-2.143607</td>\n",
       "      <td>0.434727</td>\n",
       "      <td>0.197124</td>\n",
       "      <td>-0.771035</td>\n",
       "      <td>-1.251205</td>\n",
       "      <td>4.804886</td>\n",
       "      <td>-1.478456</td>\n",
       "      <td>-0.998090</td>\n",
       "      <td>-0.922137</td>\n",
       "      <td>1.115630</td>\n",
       "      <td>-0.938156</td>\n",
       "      <td>-1.490014</td>\n",
       "      <td>0.800451</td>\n",
       "      <td>1.158040</td>\n",
       "      <td>1.194545</td>\n",
       "      <td>-0.014217</td>\n",
       "      <td>0.221277</td>\n",
       "      <td>0.129506</td>\n",
       "      <td>-0.188670</td>\n",
       "      <td>-3.000051</td>\n",
       "      <td>1.278539</td>\n",
       "      <td>-2.234303</td>\n",
       "      <td>0.474507</td>\n",
       "      <td>0.183615</td>\n",
       "      <td>-0.803547</td>\n",
       "      <td>-1.224828</td>\n",
       "      <td>5.070035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540344</td>\n",
       "      <td>-0.460631</td>\n",
       "      <td>0.574291</td>\n",
       "      <td>0.800037</td>\n",
       "      <td>0.312553</td>\n",
       "      <td>0.324191</td>\n",
       "      <td>0.364434</td>\n",
       "      <td>0.914359</td>\n",
       "      <td>0.903921</td>\n",
       "      <td>-0.837096</td>\n",
       "      <td>-0.554302</td>\n",
       "      <td>-0.518321</td>\n",
       "      <td>-0.480827</td>\n",
       "      <td>-0.766858</td>\n",
       "      <td>-0.625129</td>\n",
       "      <td>-0.484135</td>\n",
       "      <td>-0.111312</td>\n",
       "      <td>0.271020</td>\n",
       "      <td>-0.718647</td>\n",
       "      <td>-0.308095</td>\n",
       "      <td>0.824482</td>\n",
       "      <td>0.843511</td>\n",
       "      <td>-0.131282</td>\n",
       "      <td>0.081735</td>\n",
       "      <td>0.718973</td>\n",
       "      <td>-0.096461</td>\n",
       "      <td>0.929861</td>\n",
       "      <td>0.287661</td>\n",
       "      <td>0.843883</td>\n",
       "      <td>0.829564</td>\n",
       "      <td>-0.618349</td>\n",
       "      <td>-0.406887</td>\n",
       "      <td>-0.345206</td>\n",
       "      <td>0.071062</td>\n",
       "      <td>-0.281154</td>\n",
       "      <td>-0.314816</td>\n",
       "      <td>0.132433</td>\n",
       "      <td>-0.358851</td>\n",
       "      <td>0.758957</td>\n",
       "      <td>-0.752305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168785</td>\n",
       "      <td>0.841776</td>\n",
       "      <td>-0.095604</td>\n",
       "      <td>0.806774</td>\n",
       "      <td>0.441698</td>\n",
       "      <td>0.946109</td>\n",
       "      <td>0.948896</td>\n",
       "      <td>-0.231689</td>\n",
       "      <td>-0.066734</td>\n",
       "      <td>-0.052881</td>\n",
       "      <td>-0.143452</td>\n",
       "      <td>-0.132586</td>\n",
       "      <td>-0.342799</td>\n",
       "      <td>0.613181</td>\n",
       "      <td>-0.218763</td>\n",
       "      <td>0.630627</td>\n",
       "      <td>-0.843133</td>\n",
       "      <td>0.662263</td>\n",
       "      <td>0.483222</td>\n",
       "      <td>0.734136</td>\n",
       "      <td>-0.034362</td>\n",
       "      <td>-0.242981</td>\n",
       "      <td>0.854260</td>\n",
       "      <td>-0.161280</td>\n",
       "      <td>0.617310</td>\n",
       "      <td>0.457597</td>\n",
       "      <td>0.958765</td>\n",
       "      <td>0.964146</td>\n",
       "      <td>-0.344561</td>\n",
       "      <td>-0.203140</td>\n",
       "      <td>-0.189572</td>\n",
       "      <td>-0.138043</td>\n",
       "      <td>-0.261676</td>\n",
       "      <td>-0.241913</td>\n",
       "      <td>0.471713</td>\n",
       "      <td>-0.168031</td>\n",
       "      <td>0.621794</td>\n",
       "      <td>-0.878340</td>\n",
       "      <td>0.316804</td>\n",
       "      <td>0.652683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.152398</td>\n",
       "      <td>-0.424813</td>\n",
       "      <td>0.413987</td>\n",
       "      <td>1.053803</td>\n",
       "      <td>-0.134084</td>\n",
       "      <td>-0.093792</td>\n",
       "      <td>0.680361</td>\n",
       "      <td>1.116450</td>\n",
       "      <td>1.113446</td>\n",
       "      <td>-0.919693</td>\n",
       "      <td>-1.039942</td>\n",
       "      <td>-0.880702</td>\n",
       "      <td>-0.705203</td>\n",
       "      <td>-0.289388</td>\n",
       "      <td>0.394628</td>\n",
       "      <td>-0.879036</td>\n",
       "      <td>0.849662</td>\n",
       "      <td>-1.852741</td>\n",
       "      <td>-0.604088</td>\n",
       "      <td>-0.418085</td>\n",
       "      <td>-0.022004</td>\n",
       "      <td>0.023693</td>\n",
       "      <td>-0.182847</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>1.091417</td>\n",
       "      <td>-0.388545</td>\n",
       "      <td>-0.060810</td>\n",
       "      <td>0.747431</td>\n",
       "      <td>1.145136</td>\n",
       "      <td>1.140010</td>\n",
       "      <td>-1.014595</td>\n",
       "      <td>-0.842491</td>\n",
       "      <td>-0.838181</td>\n",
       "      <td>-0.148985</td>\n",
       "      <td>-1.302981</td>\n",
       "      <td>0.214283</td>\n",
       "      <td>-1.080889</td>\n",
       "      <td>0.090501</td>\n",
       "      <td>0.676464</td>\n",
       "      <td>-0.631085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791835</td>\n",
       "      <td>1.030207</td>\n",
       "      <td>-0.637567</td>\n",
       "      <td>-1.022182</td>\n",
       "      <td>0.733144</td>\n",
       "      <td>1.081171</td>\n",
       "      <td>1.085387</td>\n",
       "      <td>-0.735385</td>\n",
       "      <td>-0.639518</td>\n",
       "      <td>-0.630180</td>\n",
       "      <td>-0.157422</td>\n",
       "      <td>-0.407330</td>\n",
       "      <td>0.635625</td>\n",
       "      <td>-0.914054</td>\n",
       "      <td>0.457261</td>\n",
       "      <td>0.630627</td>\n",
       "      <td>-0.698937</td>\n",
       "      <td>-0.673768</td>\n",
       "      <td>-0.169920</td>\n",
       "      <td>-1.068716</td>\n",
       "      <td>-0.427372</td>\n",
       "      <td>-0.859253</td>\n",
       "      <td>1.043303</td>\n",
       "      <td>-0.667724</td>\n",
       "      <td>-1.098230</td>\n",
       "      <td>0.753027</td>\n",
       "      <td>1.094540</td>\n",
       "      <td>1.101288</td>\n",
       "      <td>-0.823922</td>\n",
       "      <td>-0.748485</td>\n",
       "      <td>-0.739139</td>\n",
       "      <td>-0.150193</td>\n",
       "      <td>-0.544956</td>\n",
       "      <td>0.721040</td>\n",
       "      <td>-1.027395</td>\n",
       "      <td>0.496664</td>\n",
       "      <td>0.621794</td>\n",
       "      <td>-0.728753</td>\n",
       "      <td>-0.759601</td>\n",
       "      <td>-0.014920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.637331</td>\n",
       "      <td>-0.234259</td>\n",
       "      <td>0.075567</td>\n",
       "      <td>1.025624</td>\n",
       "      <td>0.346910</td>\n",
       "      <td>0.630713</td>\n",
       "      <td>0.647568</td>\n",
       "      <td>1.103504</td>\n",
       "      <td>1.077974</td>\n",
       "      <td>-1.000276</td>\n",
       "      <td>-0.889965</td>\n",
       "      <td>-0.897726</td>\n",
       "      <td>0.103733</td>\n",
       "      <td>-0.425808</td>\n",
       "      <td>0.122693</td>\n",
       "      <td>-0.656904</td>\n",
       "      <td>-0.227794</td>\n",
       "      <td>0.865674</td>\n",
       "      <td>-0.661367</td>\n",
       "      <td>0.260186</td>\n",
       "      <td>0.673354</td>\n",
       "      <td>1.021732</td>\n",
       "      <td>1.432837</td>\n",
       "      <td>1.340695</td>\n",
       "      <td>1.076397</td>\n",
       "      <td>1.026235</td>\n",
       "      <td>0.859814</td>\n",
       "      <td>0.685027</td>\n",
       "      <td>1.160470</td>\n",
       "      <td>1.135308</td>\n",
       "      <td>-0.651067</td>\n",
       "      <td>-0.539124</td>\n",
       "      <td>-0.501940</td>\n",
       "      <td>-0.126416</td>\n",
       "      <td>-0.281154</td>\n",
       "      <td>-0.491182</td>\n",
       "      <td>-0.091565</td>\n",
       "      <td>-0.633456</td>\n",
       "      <td>0.800204</td>\n",
       "      <td>-0.691695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.486667</td>\n",
       "      <td>1.093074</td>\n",
       "      <td>-0.537593</td>\n",
       "      <td>-0.515897</td>\n",
       "      <td>0.707642</td>\n",
       "      <td>1.184280</td>\n",
       "      <td>1.169622</td>\n",
       "      <td>-0.850396</td>\n",
       "      <td>-0.818154</td>\n",
       "      <td>-0.809402</td>\n",
       "      <td>0.110336</td>\n",
       "      <td>0.142159</td>\n",
       "      <td>0.635625</td>\n",
       "      <td>-0.655200</td>\n",
       "      <td>-0.263832</td>\n",
       "      <td>1.064129</td>\n",
       "      <td>-0.843133</td>\n",
       "      <td>0.019156</td>\n",
       "      <td>-0.112437</td>\n",
       "      <td>-0.740925</td>\n",
       "      <td>0.316372</td>\n",
       "      <td>-0.557405</td>\n",
       "      <td>1.106375</td>\n",
       "      <td>-0.574302</td>\n",
       "      <td>-0.623340</td>\n",
       "      <td>0.727177</td>\n",
       "      <td>1.198193</td>\n",
       "      <td>1.185925</td>\n",
       "      <td>-0.933377</td>\n",
       "      <td>-0.918564</td>\n",
       "      <td>-0.909751</td>\n",
       "      <td>0.082694</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.721040</td>\n",
       "      <td>-0.773309</td>\n",
       "      <td>-0.212344</td>\n",
       "      <td>1.059973</td>\n",
       "      <td>-0.878340</td>\n",
       "      <td>-0.201330</td>\n",
       "      <td>0.043835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200891</td>\n",
       "      <td>0.546581</td>\n",
       "      <td>0.877087</td>\n",
       "      <td>0.436406</td>\n",
       "      <td>0.106413</td>\n",
       "      <td>2.539504</td>\n",
       "      <td>-0.005246</td>\n",
       "      <td>0.589486</td>\n",
       "      <td>0.570749</td>\n",
       "      <td>0.148025</td>\n",
       "      <td>-0.094848</td>\n",
       "      <td>0.150501</td>\n",
       "      <td>5.860762</td>\n",
       "      <td>1.006602</td>\n",
       "      <td>-0.897064</td>\n",
       "      <td>2.008684</td>\n",
       "      <td>-2.790391</td>\n",
       "      <td>1.545277</td>\n",
       "      <td>-0.775926</td>\n",
       "      <td>2.881612</td>\n",
       "      <td>-0.758728</td>\n",
       "      <td>-0.178292</td>\n",
       "      <td>-0.266570</td>\n",
       "      <td>0.506444</td>\n",
       "      <td>0.456357</td>\n",
       "      <td>-0.069078</td>\n",
       "      <td>1.490241</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>0.608150</td>\n",
       "      <td>0.595224</td>\n",
       "      <td>-0.419620</td>\n",
       "      <td>-0.597464</td>\n",
       "      <td>-0.444337</td>\n",
       "      <td>2.691873</td>\n",
       "      <td>0.302748</td>\n",
       "      <td>-0.667549</td>\n",
       "      <td>0.505762</td>\n",
       "      <td>-2.355972</td>\n",
       "      <td>1.418906</td>\n",
       "      <td>-0.812915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734001</td>\n",
       "      <td>0.524876</td>\n",
       "      <td>1.025153</td>\n",
       "      <td>5.293729</td>\n",
       "      <td>0.169810</td>\n",
       "      <td>0.640036</td>\n",
       "      <td>0.624904</td>\n",
       "      <td>-0.594738</td>\n",
       "      <td>-0.721461</td>\n",
       "      <td>-0.625011</td>\n",
       "      <td>10.992971</td>\n",
       "      <td>0.416904</td>\n",
       "      <td>-0.651775</td>\n",
       "      <td>0.263729</td>\n",
       "      <td>-2.810190</td>\n",
       "      <td>1.379404</td>\n",
       "      <td>-0.771035</td>\n",
       "      <td>5.741444</td>\n",
       "      <td>-0.073920</td>\n",
       "      <td>-0.105829</td>\n",
       "      <td>1.100872</td>\n",
       "      <td>0.649984</td>\n",
       "      <td>0.536331</td>\n",
       "      <td>0.886029</td>\n",
       "      <td>4.826022</td>\n",
       "      <td>0.181993</td>\n",
       "      <td>0.651077</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>-0.690070</td>\n",
       "      <td>-0.826503</td>\n",
       "      <td>-0.734218</td>\n",
       "      <td>9.548045</td>\n",
       "      <td>0.304885</td>\n",
       "      <td>-0.546003</td>\n",
       "      <td>0.128697</td>\n",
       "      <td>-2.716028</td>\n",
       "      <td>1.378648</td>\n",
       "      <td>-0.803547</td>\n",
       "      <td>4.408971</td>\n",
       "      <td>0.083205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    BrdIndx      Area     Round  ...  NDVI_140  BordLngth_140  GLCM3_140\n",
       "0 -1.221577 -0.675542 -0.476591  ... -0.803547      -1.224828   5.070035\n",
       "1  0.540344 -0.460631  0.574291  ... -0.878340       0.316804   0.652683\n",
       "2  0.152398 -0.424813  0.413987  ... -0.728753      -0.759601  -0.014920\n",
       "3  0.637331 -0.234259  0.075567  ... -0.878340      -0.201330   0.043835\n",
       "4  0.200891  0.546581  0.877087  ... -0.803547       4.408971   0.083205\n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QdiKjC1QIwz"
   },
   "source": [
    "### 2. Random Forest Classifier - Base Model:\n",
    "\n",
    "Start by creating a simple Random Forest only using default parameters - this will let us compare SVMs to Random Forest in multiclass problems.\n",
    "\n",
    "a) Use the RandomForestClassifier in sklearn. Fit your model on the training data.\n",
    "\n",
    "b) Use the fitted model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n",
    "c) Calculate the confusion matrix and classification report for the test data. \n",
    "\n",
    "d)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Y6bH5CImQIKS"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred_rf = rf.predict(X_train_scaled)\n",
    "y_test_pred_rf = rf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnOmKiT3hKdi",
    "outputId": "5e01bcde-b56a-45b0-ceed-fd9e94c9cc53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Score: 1.0\n",
      "\n",
      "Random Forest Train Confusion Matrix\n",
      "[[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 97  0  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  0  0 93  0  0  0  0  0]\n",
      " [ 0  0  0  0 83  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  0  0  0 89]]\n",
      "\n",
      "Random Forest Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        1.00      1.00      1.00        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        1.00      1.00      1.00        93\n",
      "      grass        1.00      1.00      1.00        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      1.00      1.00        20\n",
      "       tree        1.00      1.00      1.00        89\n",
      "\n",
      "    accuracy                           1.00       507\n",
      "   macro avg       1.00      1.00      1.00       507\n",
      "weighted avg       1.00      1.00      1.00       507\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Random Forest Test Score: 0.8333333333333334\n",
      "\n",
      "Random Forest Test Confusion Matrix\n",
      "[[14  0  0  0  0  0  0  0  0]\n",
      " [ 1 22  0  2  0  0  0  0  0]\n",
      " [ 1  1 13  0  0  0  0  0  0]\n",
      " [ 0  5  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0 25  0  0  0  4]\n",
      " [ 1  0  0  0  0 14  0  0  0]\n",
      " [ 3  0  0  0  0  0 13  0  0]\n",
      " [ 0  1  0  5  2  0  0  6  0]\n",
      " [ 0  0  0  1  1  0  0  0 15]]\n",
      "\n",
      "Random Forest Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.70      1.00      0.82        14\n",
      "   building        0.76      0.88      0.81        25\n",
      "        car        1.00      0.87      0.93        15\n",
      "   concrete        0.69      0.78      0.73        23\n",
      "      grass        0.89      0.86      0.88        29\n",
      "       pool        1.00      0.93      0.97        15\n",
      "     shadow        1.00      0.81      0.90        16\n",
      "       soil        1.00      0.43      0.60        14\n",
      "       tree        0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.83       168\n",
      "   macro avg       0.87      0.83      0.83       168\n",
      "weighted avg       0.86      0.83      0.83       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Random Forest Train Score:\", rf.score(X_train_scaled, y_train))\n",
    "print(\"\\nRandom Forest Train Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_train, y_train_pred_rf))\n",
    "print(\"\\nRandom Forest Train Classification Report\")\n",
    "print(skm.classification_report(y_train, y_train_pred_rf))\n",
    "print('-'*53 + '\\n')\n",
    "print(\"Random Forest Test Score:\", rf.score(X_test_scaled, y_test))\n",
    "print(\"\\nRandom Forest Test Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_test, y_test_pred_rf))\n",
    "print(\"\\nRandom Forest Test Classification Report\")\n",
    "print(skm.classification_report(y_test, y_test_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOsXp-Zairr9"
   },
   "source": [
    "The overall accuracy is 1 for training set and 0.83 for test set. This is a clear sign of overfitting since the model performance droped a lot when applied to new unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D19jpS0lz9o6"
   },
   "source": [
    "e) Identify the top 5 features. Feel free to print a list OR to make a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "HruZ2XrZz2gX",
    "outputId": "e48573bb-02af-46be-c938-0de44de441fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Top 5 Feature Importance of Random Forest Base Model')"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame({'feature':train.columns, 'importance':rf.feature_importances_})\n",
    "feature_importances.sort_values('importance', ascending=True, inplace=True)\n",
    "pt = feature_importances.iloc[:5,].plot.barh(x='feature',y='importance', legend=False)\n",
    "pt.set_xlabel('Feature Importance')\n",
    "pt.set_title('Top 5 Feature Importance of Random Forest Base Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QbFkFVo1AO5"
   },
   "source": [
    "### 3. LinearSVM Classifier - Base Model:\n",
    "\n",
    "Create a simple LinearSVC Classifier only using default parameters.\n",
    "\n",
    "a) Use the LinearSVC in sklearn. Fit your model on the training data.\n",
    "\n",
    "b) Use the fitted model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n",
    "c) Calculate the confusion matrix and classification report for test data.\n",
    "\n",
    "d)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wK22pO6n1vXA",
    "outputId": "137ef5ee-23c2-49c0-e841-dab3bd550d6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svc = svm.LinearSVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred_svc = svc.predict(X_train_scaled)\n",
    "y_test_pred_svc = svc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wPWn06E15kn",
    "outputId": "bb56e154-a30e-4938-dc74-6e60f237281f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVM  Train Score: 0.9940828402366864\n",
      "\n",
      "LinearSVM  Train Confusion Matrix\n",
      "[[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 97  0  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  0  0 93  0  0  0  0  0]\n",
      " [ 0  1  0  0 80  0  0  0  2]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  0  0  0 89]]\n",
      "\n",
      "LinearSVM  Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        0.99      1.00      0.99        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        1.00      1.00      1.00        93\n",
      "      grass        1.00      0.96      0.98        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      1.00      1.00        20\n",
      "       tree        0.98      1.00      0.99        89\n",
      "\n",
      "    accuracy                           0.99       507\n",
      "   macro avg       1.00      1.00      1.00       507\n",
      "weighted avg       0.99      0.99      0.99       507\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "LinearSVM  Test Score: 0.7678571428571429\n",
      "\n",
      "LinearSVM  Test Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  1  1  1  0  0  0  0]\n",
      " [ 0  2 12  0  0  0  0  0  1]\n",
      " [ 1  6  0 15  0  0  0  0  1]\n",
      " [ 0  0  0  1 24  0  2  0  2]\n",
      " [ 1  0  1  0  0 13  0  0  0]\n",
      " [ 2  0  0  0  0  0 14  0  0]\n",
      " [ 0  4  0  1  3  0  0  6  0]\n",
      " [ 0  0  0  1  6  0  0  0 10]]\n",
      "\n",
      "LinearSVM  Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.76      0.93      0.84        14\n",
      "   building        0.65      0.88      0.75        25\n",
      "        car        0.86      0.80      0.83        15\n",
      "   concrete        0.79      0.65      0.71        23\n",
      "      grass        0.71      0.83      0.76        29\n",
      "       pool        1.00      0.87      0.93        15\n",
      "     shadow        0.82      0.88      0.85        16\n",
      "       soil        1.00      0.43      0.60        14\n",
      "       tree        0.71      0.59      0.65        17\n",
      "\n",
      "    accuracy                           0.77       168\n",
      "   macro avg       0.81      0.76      0.77       168\n",
      "weighted avg       0.79      0.77      0.76       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LinearSVM  Train Score:\",svc.score(X_train_scaled, y_train))\n",
    "print(\"\\nLinearSVM  Train Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_train, y_train_pred_svc))\n",
    "print(\"\\nLinearSVM  Train Classification Report\")\n",
    "print(skm.classification_report(y_train, y_train_pred_svc))\n",
    "print('-'*60 + '\\n')\n",
    "print(\"LinearSVM  Test Score:\", svc.score(X_test_scaled, y_test))\n",
    "print(\"\\nLinearSVM  Test Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_test, y_test_pred_svc))\n",
    "print(\"\\nLinearSVM  Test Classification Report\")\n",
    "print(skm.classification_report(y_test, y_test_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrRLSkyH3NsH"
   },
   "source": [
    "The overall accuracy is 0.99 for training set and 0.77 for test set. This is a clear sign of overfitting since the model performance droped a lot when applied to new unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dpq1fLqr3gRd"
   },
   "source": [
    "### 4. Support Vector Machine Classifier + Linear Kernel + Grid Search:\n",
    "\n",
    "We will now use GridSearchCV to try various hyperparameters in a SVM with linear kernel.\n",
    "\n",
    "a) Use SVC from sklearn with kernel = \"linear\". Run the GridSearchCV using the following (SVMs run much faster than RandomForest):\n",
    "\n",
    "C: 0.01 - 10 in increments of 0.2 (consider using the np.arange() method from numpy to build out a sequence of values)\n",
    "\n",
    "Note: Feel free to try out more parameters, the above is the bare minimum for this assignment.\n",
    "\n",
    "Use 5 cross-fold and the default scoring. Please set verbose = 0 to reduce the printing (sorry to our grader for not specifying this last week!).\n",
    "\n",
    "b) Identify the best performing model:\n",
    "\n",
    ".best_params_() : This method outputs to best performing parameters\n",
    ".best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test\n",
    "\n",
    "c) Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n",
    "d) Calculate the confusion matrix and classification report for test data.\n",
    "\n",
    "e)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-KAA5423Uz8",
    "outputId": "a5e8c0e8-102a-4968-b23c-527b4352ffcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01,  0.2 ,  0.4 ,  0.6 ,  0.8 ,  1.  ,  1.2 ,  1.4 ,  1.6 ,\n",
       "        1.8 ,  2.  ,  2.2 ,  2.4 ,  2.6 ,  2.8 ,  3.  ,  3.2 ,  3.4 ,\n",
       "        3.6 ,  3.8 ,  4.  ,  4.2 ,  4.4 ,  4.6 ,  4.8 ,  5.  ,  5.2 ,\n",
       "        5.4 ,  5.6 ,  5.8 ,  6.  ,  6.2 ,  6.4 ,  6.6 ,  6.8 ,  7.  ,\n",
       "        7.2 ,  7.4 ,  7.6 ,  7.8 ,  8.  ,  8.2 ,  8.4 ,  8.6 ,  8.8 ,\n",
       "        9.  ,  9.2 ,  9.4 ,  9.6 ,  9.8 , 10.  ])"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define C\n",
    "Cs = np.arange(0.0, 10, 0.2)\n",
    "Cs[0] = 0.01\n",
    "Cs = np.append(Cs, 10)\n",
    "Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "nWmRMVqc4DLv"
   },
   "outputs": [],
   "source": [
    "parameters = {'C': Cs}\n",
    "\n",
    "svc = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsFa4DRT4X7-",
    "outputId": "372a65ff-0ba4-48b4-9003-18db19a9e0e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='linear', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': array([ 0.01,  0.2 ,  0.4 ,  0.6 ,  0.8 ,  1.  ,  1.2 ,  1.4 ,  1.6 ,\n",
       "        1.8 ,  2.  ,  2.2 ,  2.4 ,  2.6 ,  2.8 ,  3.  ,  3.2 ,  3.4 ,\n",
       "        3.6 ,  3.8 ,  4.  ,  4.2 ,  4.4 ,  4.6 ,  4.8 ,  5.  ,  5.2 ,\n",
       "        5.4 ,  5.6 ,  5.8 ,  6.  ,  6.2 ,  6.4 ,  6.6 ,  6.8 ,  7.  ,\n",
       "        7.2 ,  7.4 ,  7.6 ,  7.8 ,  8.  ,  8.2 ,  8.4 ,  8.6 ,  8.8 ,\n",
       "        9.  ,  9.2 ,  9.4 ,  9.6 ,  9.8 , 10.  ])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 128,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(svc, parameters, cv=5, refit=True, n_jobs=-1, verbose=0, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QjZGGDY48jC",
    "outputId": "01945972-36b7-4959-cd00-9952e2070b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.01}\n",
      "\n",
      "Best Model: SVC(C=0.01, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:', grid_search.best_params_)\n",
    "print()\n",
    "print('Best Model:', grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "jlRkFpM249PT"
   },
   "outputs": [],
   "source": [
    "lkgs_bm=grid_search.best_estimator_\n",
    "y_train_pred_svc_lkgs = lkgs_bm.predict(X_train_scaled)\n",
    "y_test_pred_svc_lkgs = lkgs_bm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpLb5ofl5ZoP",
    "outputId": "8739163e-c2c0-4954-de5c-048d66acd9c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC w. Linear Kernel & GridSearchCV  Train Score: 0.8875739644970414\n",
      "\n",
      "SVC w. Linear Kernel & GridSearchCV  Train Confusion Matrix\n",
      "[[40  0  0  0  0  0  5  0  0]\n",
      " [ 2 87  0  7  0  0  1  0  0]\n",
      " [ 0  1 19  1  0  0  0  0  0]\n",
      " [ 0  9  0 83  1  0  0  0  0]\n",
      " [ 0  1  0  0 70  0  0  0 12]\n",
      " [ 0  1  0  0  1 12  0  0  0]\n",
      " [ 1  0  0  0  0  0 43  0  1]\n",
      " [ 0  3  0  4  2  0  0 11  0]\n",
      " [ 0  0  0  0  3  0  1  0 85]]\n",
      "\n",
      "SVC w. Linear Kernel & GridSearchCV Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.89      0.91        45\n",
      "   building        0.85      0.90      0.87        97\n",
      "        car        1.00      0.90      0.95        21\n",
      "   concrete        0.87      0.89      0.88        93\n",
      "      grass        0.91      0.84      0.88        83\n",
      "       pool        1.00      0.86      0.92        14\n",
      "     shadow        0.86      0.96      0.91        45\n",
      "       soil        1.00      0.55      0.71        20\n",
      "       tree        0.87      0.96      0.91        89\n",
      "\n",
      "    accuracy                           0.89       507\n",
      "   macro avg       0.92      0.86      0.88       507\n",
      "weighted avg       0.89      0.89      0.89       507\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "SVC w. Linear Kernel & GridSearchCV Test Score: 0.8214285714285714\n",
      "\n",
      "SVC w. Linear Kernel & GridSearchCV Test Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  0  2  1  0  0  0  0]\n",
      " [ 0  1 14  0  0  0  0  0  0]\n",
      " [ 0  5  0 17  0  0  0  1  0]\n",
      " [ 0  0  0  1 25  0  0  0  3]\n",
      " [ 0  0  0  0  0 14  1  0  0]\n",
      " [ 1  0  0  0  0  0 15  0  0]\n",
      " [ 0  3  0  5  2  0  0  4  0]\n",
      " [ 0  0  0  1  2  0  0  0 14]]\n",
      "\n",
      "SVC w. Linear Kernel & GridSearchCV Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.71      0.88      0.79        25\n",
      "        car        1.00      0.93      0.97        15\n",
      "   concrete        0.65      0.74      0.69        23\n",
      "      grass        0.83      0.86      0.85        29\n",
      "       pool        1.00      0.93      0.97        15\n",
      "     shadow        0.88      0.94      0.91        16\n",
      "       soil        0.80      0.29      0.42        14\n",
      "       tree        0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.82       168\n",
      "   macro avg       0.85      0.81      0.82       168\n",
      "weighted avg       0.83      0.82      0.81       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSVC w. Linear Kernel & GridSearchCV  Train Score:\", lkgs_bm.score(X_train_scaled, y_train))\n",
    "print(\"\\nSVC w. Linear Kernel & GridSearchCV  Train Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_train, y_train_pred_svc_lkgs))\n",
    "print(\"\\nSVC w. Linear Kernel & GridSearchCV Train Classification Report\")\n",
    "print(skm.classification_report(y_train, y_train_pred_svc_lkgs))\n",
    "print('-'*60 + '\\n')\n",
    "print(\"SVC w. Linear Kernel & GridSearchCV Test Score:\", lkgs_bm.score(X_test_scaled, y_test))\n",
    "print(\"\\nSVC w. Linear Kernel & GridSearchCV Test Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_test, y_test_pred_svc_lkgs))\n",
    "print(\"\\nSVC w. Linear Kernel & GridSearchCV Test Classification Report\")\n",
    "print(skm.classification_report(y_test, y_test_pred_svc_lkgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sgsMa9K8bz6"
   },
   "source": [
    "The overall accuracy is 0.89 for training set and 0.81 for test set. There is less overfitting in this model as compare to the basic linear SVM model and Random Forest model. Because the model performance did not change as much when applied to new unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNhHUo4n9IgK"
   },
   "source": [
    "### 5. Support Vector Machine Classifier + Polynomial Kernel + Grid Search:\n",
    "\n",
    "We will now use GridSearchCV to try various hyperparameters in a SVM with a polynomial kernel.\n",
    "\n",
    "a) Use SVC from sklearn with kernel = \"poly\". Run the GridSearchCV using the following:\n",
    "\n",
    "C: 0.01 - 10 in increments of 0.2\n",
    "degree: 2, 3, 4, 5, 6\n",
    "\n",
    "Note: Feel free to try out more parameters, the above is the bare minimum for this assignment.\n",
    "\n",
    "Use 5 cross-fold and the default scoring.\n",
    "\n",
    "b) Identify the best performing model:\n",
    "\n",
    ".best_params_() : This method outputs to best performing parameters\n",
    ".best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test\n",
    "\n",
    "c) Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n",
    "d) Calculate the confusion matrix and classification report for test data.\n",
    "\n",
    "e)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "A9SLpPW99Ulx"
   },
   "outputs": [],
   "source": [
    "svc_poly = svm.SVC(kernel='poly')\n",
    "\n",
    "parameters = {\n",
    "    'C': Cs,\n",
    "    'degree': [2, 3, 4, 5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-8oe9Rx9gel",
    "outputId": "40956822-3253-4c33-f2a1-926cfc6318d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 255 candidates, totalling 1275 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 888 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1275 out of 1275 | elapsed:   51.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='poly', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': array([ 0.01,  0.2 ,  0.4 ,  0.6 ,  0.8 ,  1.  ,  1.2 ,  1.4 ,  1.6 ,\n",
       "        1.8 ,  2.  ,  2.2 ,  2.4 ,  2.6 ,  2.8 ,  3.  ,  3.2 ,  3.4 ,\n",
       "        3.6 ,  3.8 ,  4.  ,  4.2 ,  4.4 ,  4.6 ,  4.8 ,  5.  ,  5.2 ,\n",
       "        5.4 ,  5.6 ,  5.8 ,  6.  ,  6.2 ,  6.4 ,  6.6 ,  6.8 ,  7.  ,\n",
       "        7.2 ,  7.4 ,  7.6 ,  7.8 ,  8.  ,  8.2 ,  8.4 ,  8.6 ,  8.8 ,\n",
       "        9.  ,  9.2 ,  9.4 ,  9.6 ,  9.8 , 10.  ]),\n",
       "                         'degree': [2, 3, 4, 5, 6]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 179,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_svc_poly = GridSearchCV(svc_poly, parameters, cv=5, refit=True, n_jobs=-1, verbose=1)\n",
    "grid_search_svc_poly.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qd9UEkOc97kS",
    "outputId": "e39280da-15a8-4e50-d361-9de8a913ae65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 4.4, 'degree': 3}\n",
      "\n",
      "Best Model: SVC(C=4.4, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:', grid_search_svc_poly.best_params_)\n",
    "print()\n",
    "print('Best Model:', grid_search_svc_poly.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "97pY8yg-_Of8"
   },
   "outputs": [],
   "source": [
    "pkgs_bm=grid_search_svc_poly.best_estimator_\n",
    "y_train_pred_svc_pkgs = pkgs_bm.predict(X_train_scaled)\n",
    "y_test_pred_svc_pkgs = pkgs_bm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBI6WM1s_aGi",
    "outputId": "16a92461-4297-4427-c89d-02f47edad52e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC w. Poly Kernel & GridSearchCV  Train Score: 0.9585798816568047\n",
      "\n",
      "SVC w. Poly Kernel & GridSearchCV  Train Confusion Matrix\n",
      "[[44  0  0  0  1  0  0  0  0]\n",
      " [ 0 95  0  1  1  0  0  0  0]\n",
      " [ 0  0 20  0  1  0  0  0  0]\n",
      " [ 0  1  0 91  1  0  0  0  0]\n",
      " [ 0  1  0  0 81  0  0  0  1]\n",
      " [ 0  0  0  0  1 13  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  9  0  0 11  0]\n",
      " [ 0  0  0  0  3  0  0  0 86]]\n",
      "\n",
      "SVC w. Poly Kernel & GridSearchCV Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      0.98      0.99        45\n",
      "   building        0.98      0.98      0.98        97\n",
      "        car        1.00      0.95      0.98        21\n",
      "   concrete        0.99      0.98      0.98        93\n",
      "      grass        0.83      0.98      0.90        83\n",
      "       pool        1.00      0.93      0.96        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      0.55      0.71        20\n",
      "       tree        0.99      0.97      0.98        89\n",
      "\n",
      "    accuracy                           0.96       507\n",
      "   macro avg       0.98      0.92      0.94       507\n",
      "weighted avg       0.96      0.96      0.96       507\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "SVC w. Poly Kernel & GridSearchCV Test Score: 0.7738095238095238\n",
      "\n",
      "SVC w. Poly Kernel & GridSearchCV Test Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  0  2  1  0  0  0  0]\n",
      " [ 0  2 11  0  0  1  0  1  0]\n",
      " [ 0  5  0 17  1  0  0  0  0]\n",
      " [ 0  0  0  0 26  0  0  1  2]\n",
      " [ 0  0  0  0  0 14  1  0  0]\n",
      " [ 1  0  0  0  0  0 14  0  1]\n",
      " [ 0  3  0  5  6  0  0  0  0]\n",
      " [ 0  0  0  1  3  0  0  0 13]]\n",
      "\n",
      "SVC w. Poly Kernel & GridSearchCV Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.69      0.88      0.77        25\n",
      "        car        1.00      0.73      0.85        15\n",
      "   concrete        0.68      0.74      0.71        23\n",
      "      grass        0.70      0.90      0.79        29\n",
      "       pool        0.93      0.93      0.93        15\n",
      "     shadow        0.88      0.88      0.88        16\n",
      "       soil        0.00      0.00      0.00        14\n",
      "       tree        0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.77       168\n",
      "   macro avg       0.74      0.75      0.74       168\n",
      "weighted avg       0.73      0.77      0.75       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSVC w. Poly Kernel & GridSearchCV  Train Score:\", pkgs_bm.score(X_train_scaled, y_train))\n",
    "print(\"\\nSVC w. Poly Kernel & GridSearchCV  Train Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_train, y_train_pred_svc_pkgs))\n",
    "print(\"\\nSVC w. Poly Kernel & GridSearchCV Train Classification Report\")\n",
    "print(skm.classification_report(y_train, y_train_pred_svc_pkgs))\n",
    "print('-'*60 + '\\n')\n",
    "print(\"SVC w. Poly Kernel & GridSearchCV Test Score:\", pkgs_bm.score(X_test_scaled, y_test))\n",
    "print(\"\\nSVC w. Poly Kernel & GridSearchCV Test Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_test, y_test_pred_svc_pkgs))\n",
    "print(\"\\nSVC w. Poly Kernel & GridSearchCV Test Classification Report\")\n",
    "print(skm.classification_report(y_test, y_test_pred_svc_pkgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuDb36SPBiJq"
   },
   "source": [
    "The overall accuracy is 0.96 for training set and 0.77 for test set. The overfitting in this model is worse compare to best linear kernal gridsearch model. The model performance dropped a lot when applied to new unseen test data, we consider this model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gIE9N6CCdDz"
   },
   "source": [
    "### 6. Support Vector Machine Classifier + RBF Kernel + Grid Search:\n",
    "\n",
    "We will now use GridSearchCV to try various hyperparameters in a SVM with a RBF kernel.\n",
    "\n",
    "a) Use SVC from sklearn with kernel = \"rbf\". Run the GridSearchCV using the following:\n",
    "\n",
    "C: 0.01 - 10 in increments of 0.2\n",
    "gamma: 0.01,  0.1, 1, 10, 100\n",
    "\n",
    "Note: Feel free to try out more parameters, the above is the bare minimum for this assignment.\n",
    "\n",
    "Use 5 cross-fold and the default scoring.\n",
    "\n",
    "b) Identify the best performing model:\n",
    "\n",
    ".best_params_() : This method outputs to best performing parameters\n",
    ".best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test\n",
    "\n",
    "c) Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n",
    "d) Calculate the confusion matrix and classification report for test data.\n",
    "\n",
    "e)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "XNPlVLfVCcWA"
   },
   "outputs": [],
   "source": [
    "svc_rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "parameters = {\n",
    "    'C': Cs,\n",
    "    'gamma': [0.01, 0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sNSDKzhQFXgU",
    "outputId": "8166759e-0e7b-4ed9-abf7-a0a74d52e5c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 255 candidates, totalling 1275 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done 888 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1275 out of 1275 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': array([ 0.01,  0.2 ,  0.4 ,  0.6 ,  0.8 ,  1.  ,  1.2 ,  1.4 ,  1.6 ,\n",
       "        1.8 ,  2.  ,  2.2 ,  2.4 ,  2.6 ,  2.8 ,  3.  ,  3.2 ,  3.4 ,\n",
       "        3.6 ,  3.8 ,  4.  ,  4.2 ,  4.4 ,  4.6 ,  4.8 ,  5.  ,  5.2 ,\n",
       "        5.4 ,  5.6 ,  5.8 ,  6.  ,  6.2 ,  6.4 ,  6.6 ,  6.8 ,  7.  ,\n",
       "        7.2 ,  7.4 ,  7.6 ,  7.8 ,  8.  ,  8.2 ,  8.4 ,  8.6 ,  8.8 ,\n",
       "        9.  ,  9.2 ,  9.4 ,  9.6 ,  9.8 , 10.  ]),\n",
       "                         'gamma': [0.01, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 167,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_svc_rbf = GridSearchCV(svc_rbf, parameters, cv=5, refit=True, n_jobs=-1, verbose=1)\n",
    "grid_search_svc_rbf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtuX3br1Fm5m",
    "outputId": "188b134c-f728-4dba-d219-1ae085e453c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 2.8000000000000003, 'gamma': 0.01}\n",
      "\n",
      "Best Model: SVC(C=2.8000000000000003, break_ties=False, cache_size=200, class_weight=None,\n",
      "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.01,\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:', grid_search_svc_rbf.best_params_)\n",
    "print()\n",
    "print('Best Model:', grid_search_svc_rbf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "RdMSWVElFyl5"
   },
   "outputs": [],
   "source": [
    "rkgs_bm=grid_search_svc_rbf.best_estimator_\n",
    "y_train_pred_svc_rkgs = rkgs_bm.predict(X_train_scaled)\n",
    "y_test_pred_svc_rkgs = rkgs_bm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUh-0PAmF7pF",
    "outputId": "5c4ab386-4ad5-4f68-d9a8-0738ea1b6277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC w. RBF Kernel & GridSearchCV  Train Score: 0.9881656804733728\n",
      "\n",
      "SVC w. RBF Kernel & GridSearchCV  Train Confusion Matrix\n",
      "[[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 96  0  1  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  1  0 92  0  0  0  0  0]\n",
      " [ 0  1  0  0 81  0  0  0  1]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  1  0  0  0  0  0 19  0]\n",
      " [ 0  0  0  0  1  0  0  0 88]]\n",
      "\n",
      "SVC w. RBF Kernel & GridSearchCV Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        0.97      0.99      0.98        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        0.99      0.99      0.99        93\n",
      "      grass        0.99      0.98      0.98        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      0.95      0.97        20\n",
      "       tree        0.99      0.99      0.99        89\n",
      "\n",
      "    accuracy                           0.99       507\n",
      "   macro avg       0.99      0.99      0.99       507\n",
      "weighted avg       0.99      0.99      0.99       507\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "SVC w. RBF Kernel & GridSearchCV Test Score: 0.8452380952380952\n",
      "\n",
      "SVC w. RBF Kernel & GridSearchCV Test Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 21  0  3  1  0  0  0  0]\n",
      " [ 0  1 14  0  0  0  0  0  0]\n",
      " [ 0  4  0 19  0  0  0  0  0]\n",
      " [ 0  1  0  0 26  0  0  0  2]\n",
      " [ 0  0  0  0  0 14  1  0  0]\n",
      " [ 1  0  0  0  0  0 15  0  0]\n",
      " [ 0  2  0  4  3  0  0  5  0]\n",
      " [ 0  0  0  1  1  0  0  0 15]]\n",
      "\n",
      "SVC w. RBF Kernel & GridSearchCV Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.72      0.84      0.78        25\n",
      "        car        1.00      0.93      0.97        15\n",
      "   concrete        0.70      0.83      0.76        23\n",
      "      grass        0.84      0.90      0.87        29\n",
      "       pool        1.00      0.93      0.97        15\n",
      "     shadow        0.88      0.94      0.91        16\n",
      "       soil        1.00      0.36      0.53        14\n",
      "       tree        0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.85       168\n",
      "   macro avg       0.88      0.84      0.84       168\n",
      "weighted avg       0.86      0.85      0.84       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSVC w. RBF Kernel & GridSearchCV  Train Score:\", rkgs_bm.score(X_train_scaled, y_train))\n",
    "print(\"\\nSVC w. RBF Kernel & GridSearchCV  Train Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_train, y_train_pred_svc_rkgs))\n",
    "print(\"\\nSVC w. RBF Kernel & GridSearchCV Train Classification Report\")\n",
    "print(skm.classification_report(y_train, y_train_pred_svc_rkgs))\n",
    "print('-'*60 + '\\n')\n",
    "print(\"SVC w. RBF Kernel & GridSearchCV Test Score:\", rkgs_bm.score(X_test_scaled, y_test))\n",
    "print(\"\\nSVC w. RBF Kernel & GridSearchCV Test Confusion Matrix\")\n",
    "print(skm.confusion_matrix(y_test, y_test_pred_svc_rkgs))\n",
    "print(\"\\nSVC w. RBF Kernel & GridSearchCV Test Classification Report\")\n",
    "print(skm.classification_report(y_test, y_test_pred_svc_rkgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m-K2u9UIxl1"
   },
   "source": [
    "The overall accuracy is 0.99 for training set and 0.85 for test set. The rbf best model has the highest test accuracy. But there is still a sign of overfitting since the model performance still dropped a considerable amount when applied to new unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBpO_MDPKQxt"
   },
   "source": [
    "### **7. Conceptual Questions:**\n",
    "\n",
    "#### a) From the models run in steps 2-6, which performs the best based on the Classification Report? Support your reasoning with evidence around your test data. \n",
    "\n",
    "*Based on the test set Classification Report, the best rbf model in step 6 has the highest accuracy of 0.85, hence it is the best model.  However the best linear kernel with grid search in C model in step 3 has a very close test set accuracy, 0.82. We can see that the gamma step 6 grid search chose is 0.01. The gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. A very low gamma(0.01) resembles resembles linear kernel when data points are clustered well. The basic linear svm model has lower test accuracy than linear kernel model with grid search in C. With the correct kernel, properly tunning C can significantly improve model generalizability performance, hence reduce overfitting.*\n",
    "\n",
    "#### b) Compare models run for steps 4-6 where different kernels were used. What is the benefit of using a polynomial or rbf kernel over a linear kernel? What could be a downside of using a polynomial or rbf kernel? \n",
    "\n",
    "*Kernels take data as input and transform it into the required form to better seperate data in hyper-dimensions.The linear kernel is better suited for a clear linear hyper-plane that separates different classes. The poly kernel may work better in cases involving curved hyper-planes. The RBF kernel is designed to work with more localized hyper-planes, like a circle in side a ring or any cluster that locally mixed in other clusters. The gamma parameter in RBF defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. A very low gamma resembles resembles linear kernel when data points are separated well by linear hyper-planes. Depending on how classes are seperated in the data (linear, curve or localized clustering), preoper kernels should be selected accordingly. If a non-linear kernal is selected for linear problem, the performance will drop similar to what we see in the assignment. The downside of poly and RBF kernal is that they may not work well with linear hyper-planes and they are computationally more expensive* \n",
    "\n",
    "#### c) Explain the 'C' parameter used in steps 4-6. What does a small C mean versus a large C in sklearn? Why is it important to use the 'C' parameter when fitting a model? \n",
    "\n",
    "*The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. As it becomes smaller the model become more tolerant of individual errors in classifying data points, and as it become larger the model is LESS tolerant of individual errors.*\n",
    "\n",
    "*C is tuned to increase a classifier's generalizability (performance on test data). Usually one cannot draw a clean line between different classes of data. Some points will be within the Classification Margin (the distance between the decision boundary and the closest data point in a class) or misclassified (on the wrong side of the decision boundary). How tolerant the model is of these when fitting on the training data will influence not only the width of the margin but also where the decision boundary is drawn (because it is influenced by the maximum width one can have before hitting the first data point in a class).*\n",
    "\n",
    "*The reason we tune the C parameter when fitting the model is to prevent overfitting. Minimizing error on training data at\n",
    "the cost of increasing test error is overfitting. If we are intolerant of classification errors on our training data(selecting large C) at the expense of increasing classification errors on our test data, we will cause overfitting.*\n",
    "\n",
    "#### d) Scaling our input data does not matter much for Random Forest, but it is a critical step for Support Vector Machines. Explain why this is such a critical step. Also, provide an example of a feature from this data set that could cause issues with our SVMs if not scaled.\n",
    "\n",
    "*Because Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal hyperplane is influenced by the scale of the input features. If a feature is disproportionally larger in scale than the rest of the data set, this feature will overshadow all other features in creating the decision vector resulting other features' influence on minimizing decision vector becoming negligible.*\n",
    "\n",
    "*I will use an example to demonstrate. If we have two input vectors: X1 and X2. and let's say X1 has range(0.1 to 0.8) and X2 has range(3000 to 50000). Now your SVM classifier will be a linear boundary lying in X1-X2 plane. The slope of linear decision boundary should not depend on the range of X1 and X2, but instead upon the distribution of points. Now let's make a prediction on the point (0.1, 4000) and (0.8, 4000). There will be hardly any difference in the value of the function, thus making SVM less accurate since it will have way less sensitivity to points in the X1 direction.*\n",
    "\n",
    "*To pick an example of a feature that can cause issues on SVM, I will find the maximum of all feature means and the average of those means.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-7GUkAyvDwi",
    "outputId": "95a4a99b-6ecd-4484-a805-9584a007655a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the entire data set is 336.0132799313019\n",
      "Feature with the maximun mean is Area_140 with mean of 7982.4812623274165\n"
     ]
    }
   ],
   "source": [
    "mean_c=[]\n",
    "rag_c=[]\n",
    "for i in range(len(train.columns)):\n",
    "   mean_c.append(np.mean(train.iloc[:,i]))\n",
    "print('The mean of the entire data set is', np.mean(mean_c))\n",
    "max_ind=mean_c.index(max(mean_c))\n",
    "print('Feature with the maximun mean is', train.columns[max_ind], 'with mean of',max(mean_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZH5VZXserzo"
   },
   "source": [
    "*We can see that the feature Area_140 has a scale with an order of magnitude larger than the average scale of the data set. This feature will definitely cause issue if not scaled*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZQzcQswdfy3"
   },
   "source": [
    "#### e) Describe conceptually what the purpose of a kernel is for Support Vector Machines.\n",
    "\n",
    " *In machine learning, a “kernel” is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem. It entails transforming linearly inseparable data to linearly separable ones. The kernel function is what is applied on each data point to map the original non-linear observations into a higher-dimensional space in which they become separable.*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment5_Duo_Zhou.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
