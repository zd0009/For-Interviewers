---
title: "Linear&NonLinear_Course_Project"
author: "Duo Zhou"
date: "8/14/2020"
output: html_document
---
## Step 1
```{r}
library(dplyr)
library(MASS)
library(pscl)
```

### 1 Problem Description
Business analytics group of a manufacturing company is asked to investigate causes of malfunctions in technological process at one of the plants that result in significant increase of cost for the end product of the business.
One of suspected reasons for malfunctions is deviation of temperature in burning zone from optimal levels. The sample in the provided file contains times of malfunctions in seconds since the start of measurement and records of temperature.

### 2 Data
File MScA_LinearNonLinear_CourseProject.csv contains time stamps of malfunction events expressed in seconds and temperature readings at the time of each event.
Temperature sensor updates readings once a minute.

Read and prepare the data.

```{r}
dataPath<-"C:/Users/zd000/Desktop/MSCA/Linear&NonLinear/Course_Project/"
Course.Project.Data <-read.csv(paste(dataPath,"Course_Project.csv",
                                       sep="/"),header=T)
counting_process<-as.data.frame(cbind(Time=Course.Project.Data$Time,                                      Count=1:length(Course.Project.Data$Time)))
counting_process[1:10,]
plot(counting_process$Time,counting_process$Count,type="s")
```
The counting process trajectory looks pretty smooth and grows steadily. This may mean that malfunctions are caused by a persistent rather than episodic problem.

### 2.2 Cumulative intensity
Explore cumulative intensity of the process.

Cumulative intensity is calculated as Λ(t)=Nt/t, where Nt is the number of events during the time interval [0,t].
For this project t is sequence of time stamps and Nt is event count up until t.

```{r}
plot(counting_process$Time,
     counting_process$Count/counting_process$Time,type="l",
     ylab="Cumulative Intensity")
abline(h=counting_process$Count[length(counting_process$Count)]/
         counting_process$Time[length(counting_process$Time)])
abline(h=mean(counting_process$Count/counting_process$Time))
```

The two horizontal lines on the graph are at the mean cumulative intensity and last cumulative intensity levels.

The cumulative intensity seems to converge to a stable level.

```{r}
c(Last.Intensity=counting_process$Count[length(counting_process$Count)]/
         counting_process$Time[length(counting_process$Time)],
  Mean.Intensity=mean(counting_process$Count/counting_process$Time))
```

### 2.3 Minute data
Create dataframe ‘minute_data’ containing one-minute event counts and temperatures.

For example, look at the first rows of the data:

```{r}
Course.Project.Data[1:25,]
```

Time column of the data is in seconds.
First 7 rows (events) occurred during the first minute: time stamps are between 0 and 60.
Temperature during the first minute was 96.3233199°F.
The following 15 rows are in the second minute and the second minute temperature is 100.4779092°F.

Create dataframe ‘minute_data’. Each row corresponds to one minute of observations, it contains count of events during that minute and the temperature.

Some minutes may not contain any events. Rows corresponding to such minutes should be excluded from the dataframe.
```{r}
Course.Project.Data$minute<-floor(Course.Project.Data$Time/60)+1
minute_data<-Course.Project.Data %>% 
  group_by(minute) %>%
  summarize( count=n(), temperature = mean(Temperature)) 

minute_data[minute_data['minute']==229,]
```

### Step 2
```{r}
test_dat <- read.table(paste(dataPath,'step2_sample.csv',sep = '/'), header=TRUE)
```

Method 1
```{r}
Test.Deviance.Overdispersion.Poisson<-function(my.Sample){
  Model<-glm(my.Sample~1,family=poisson)
  Dev<-Model$deviance
  Deg.Fred<-Model$df.residual
  qch<-qchisq(c(.025,.975), df=Deg.Fred, lower.tail=FALSE)
  upper.limit<-(Deg.Fred)+sqrt(2*Deg.Fred)*1.96
  lower.limit<-(Deg.Fred)-sqrt(2*Deg.Fred)*1.96
  ans<-((Dev-Deg.Fred)/sqrt(2*Deg.Fred)>-1.96)&((Dev-Deg.Fred)/sqrt(2*Deg.Fred)<1.96)
  return(cbind(Dev, upper.limit,lower.limit, log=ans,qch[1],qch[2]))
} 
Test.Deviance.Overdispersion.Poisson(test_dat$count)
```

```{r}
Test.Deviance.Overdispersion.Poisson(minute_data$count)
```

Method 2
```{r}
AER::dispersiontest(glm(test_dat$count~1,family="poisson"),alternative="two.sided")
```

```{r}
AER::dispersiontest(glm(minute_data$count~1,family="poisson"),alternative="two.sided")
```

```{r}
NB.model.pois<-glm.nb(test_dat$count~1)
odTest(NB.model.pois)
```

```{r}
NB.model.pois1<-glm.nb(minute_data$count~1)
odTest(NB.model.pois1)
```

### 4 Analyses of distributions of variables
4.1 Find distribution of Poisson intensity.
4.1.1 Kolmlgorov-Smirnov test.
Kolmogorov-Smirnov test is used to test hypotheses of equivalence between two empirical distributions or equivalence between one empirical distribution and one theoretical distribution.

```{r}
library(lattice)
library(latticeExtra)
```

```{r}
fitExp <- MASS::fitdistr(counting_process$Time, "exponential")
KSTest_event_intervals <- ks.test(counting_process$Time, "pexp", rate=fitExp$estimate)
c(KSTest_event_intervals$statistic,p.value=KSTest_event_intervals$p.value)
```


Intensity 

```{r}
event_intensities = minute_data$count
# Normal
fit.norm = MASS::fitdistr(event_intensities, densfun='normal')
ks.test(event_intensities,"pnorm",mean=fit.norm$estimate[1],sd=fit.norm$estimate[2])
# D = 0.15707, p-value = 8.78e-06

# Exponential
fit.exp = MASS::fitdistr(event_intensities, densfun = "exponential")
ks.test(event_intensities, "pexp", rate = fit.exp$estimate)
# D = 0.17956, p-value = 1.996e-07

# Gamma
fit.gam = MASS::fitdistr(event_intensities, densfun="gamma")
ks.test(event_intensities, "pgamma", shape=fit.gam$estimate[1], rate=fit.gam$estimate[2])
# D = 0.076201, p-value = 0.1097

# Log Normal
fit.lognorm = MASS::fitdistr(event_intensities, densfun = "lognormal")
ks.test(event_intensities, "plnorm", meanlog=fit.lognorm$estimate[1], sdlog=fit.lognorm$estimate[2])
# D = 0.12003, p-value = 0.001488

# Logistic
fit.logit = MASS::fitdistr(event_intensities, densfun="logistic")
ks.test(event_intensities, "plogis", location=fit.logit$estimate[1], scale=fit.logit$estimate[2])
# D = 0.10735, p-value = 0.006287
```

Temperature

```{r}
# Fit the same five candidates on temperature
temps = minute_data$temperature

# Normal
fit.norm = MASS::fitdistr(temps, densfun='normal')
ks.test(temps,"pnorm",mean=fit.norm$estimate[1],sd=fit.norm$estimate[2])
# D = 0.041953, p-value = 0.7711

# Exponential
fit.exp = MASS::fitdistr(temps, densfun = "exponential")
ks.test(temps, "pexp", rate = fit.exp$estimate)
# D = 0.57975, p-value < 2.2e-16

# Gamma
fit.gam = MASS::fitdistr(temps, densfun="gamma")
ks.test(temps, "pgamma", shape=fit.gam$estimate[1], rate=fit.gam$estimate[2])
# D = 0.048746, p-value = 0.5924

# Log Normal
fit.lognorm = MASS::fitdistr(temps, densfun = "lognormal")
ks.test(temps, "plnorm", meanlog=fit.lognorm$estimate[1], sdlog=fit.lognorm$estimate[2])
# D = 0.052065, p-value = 0.5069

# Logistic
fit.logit = MASS::fitdistr(temps, densfun="logistic")
ks.test(temps, "plogis", location=fit.logit$estimate[1], scale=fit.logit$estimate[2])
# D = 0.038929, p-value = 0.8431

```

```{r}
fit.logit$estimate[1]
fit.logit$estimate[2]
```


```{r}
fit.norm$estimate[1]
fit.norm$estimate[2]
```


```{r}
fit.gam$estimate[1]
fit.gam$estimate[2]
```

```{r}
fit.lognorm$estimate[1]
fit.lognorm$estimate[2]
```
```{r}
fit.intensity.gam = MASS::fitdistr(event_intensities, densfun="gamma")
fit.temp.norm = MASS::fitdistr(temps, densfun='normal')
distrParam <- list(intensity=fit.intensity.gam, temperature=fit.temp.norm)
distrParam
```






```{r}
part2_data<-minute_data
head(part2_data)
```

```{r}
library(copula)
```

```{r}
part2_data<-part2_data[complete.cases(part2_data),]
part2_data$intensity<-part2_data$count
head(part2_data)
```

```{r}
plot(part2_data$temperature,part2_data$intensity)
```


```{r}
plot(rank(part2_data$temperature)/(dim(part2_data)[1]+1),
     rank(part2_data$intensity)/(dim(part2_data)[1]+1),xlab="Temperature",ylab="Intensity")
```

```{r}
plot(pobs(part2_data[,c("temperature","intensity")]),xlab="Temperature",ylab="Intensity")
```

# Part 1 - Fit Parametric Copulas
Normal, Gumbel, Clayton, Frank
Compare likelihood values for different parametric copulas and select the best of them
```{r}

copula_data <- copula::pobs(part2_data[,c("temperature","intensity")])

fit.copula.normal <- fitCopula(
  copula = normalCopula(),
  data = copula_data,
  method = "ml",
  optim.method = "BFGS")
fit.copula.normal@loglik
# 198.146

fit.copula.gumbel <- fitCopula(
  copula = gumbelCopula(),
  data = copula_data,
  method = "ml",
  optim.method = "BFGS")
fit.copula.gumbel@loglik
# 247.092

fit.copula.clayton <- fitCopula(
  copula = claytonCopula(),
  data = 1-copula_data,
  method = "ml",
  optim.method = "BFGS")
fit.copula.clayton@loglik
# 111.659 -- wrong for some reason
# 259.973

fit.copula.frank <- fitCopula(
  copula = frankCopula(),
  data = copula_data,
  method = "ml",
  optim.method = "BFGS")
fit.copula.frank@loglik
# 211.514

```
2 Copula quantile regression
Plot the selected parametric copula using functions persp() and contour()

Recall marginal distributions of the data. They play important role in copula quantile regression.

Load parameters of the marginal distributions of temperature and event intensity fitted to the data in Part 1.

```{r}
alphaClayton = summary(fit.copula.clayton)$coefficients[1]
copulaClayton_sim <- claytonCopula(param=alphaClayton,dim=2)
persp(copulaClayton_sim, dCopula, main="pdf",xlab="u", ylab="v", zlab="c(u,v)")

contour(copulaClayton_sim,dCopula, main="pdf",xlab="u", ylab="v")

simData<-rCopula(1000,copulaClayton_sim)
plot(simData)
k <- kde2d(copula_data[,1], copula_data[,2], n=200)
image(k,col=rainbow(40))

```

```{r}
# Follow the steps of copula quantile regression:

# 1. Calculate copula image of the predictor imgX using the marginal 
#    distribution fitted to the data in Part 1
temp_mean = distrParam$temperature$estimate[1]
temp_sd = distrParam$temperature$estimate[2]

imgX <- pnorm(part2_data$temperature, mean=temp_mean, sd=temp_sd)

# 2. If the copula needs to be inverted from upper tail to lower tail or 
#    vice versa, rotate the image by imgX <- -imgX+1
copulat_data_rot = 1-copula_data
imgX <- 1-imgX


# 3. Calculate image quantiles of conditional distribution V|U corresponding 
#    to the selected copula type, its parameter and every point of
#    imgX (image of X) for levels 5%, 95% and 50% (median). Use formulas given
#    in the lecture for quantiles of conditional distributions corresponding to 
#    different copula types

qclayton <- function(X, alpha_, theta_) {
  # Clayton conditional distribution quantile
  ((alpha_**(-theta_/(1+theta_)) - 1) * X**(-theta_) + 1) ** (-1/theta_)
}

theta <- fit.copula.clayton@estimate

intensity.Q.Low  <- qclayton(imgX, alpha_=0.95, theta_=theta)
intensity.Q.Mid  <- qclayton(imgX, alpha_=0.50, theta_=theta)
intensity.Q.High <- qclayton(imgX, alpha_=0.05, theta_=theta)

plot(cbind(imgX, copulat_data_rot[,2]))
points(imgX,intensity.Q.Low,col="orange",pch=16)
points(imgX,intensity.Q.Mid,col="cyan",pch=16)
points(imgX,intensity.Q.High,col="orange")

# 4. Reconstruct quantiles for the original data units from the image quantiles
#   using marginal distributions fitted to the variables. Remember to rotate 
#   image before reconstructing if necessary

intensity.Q.Low  = 1 - intensity.Q.Low
intensity.Q.Mid  = 1 - intensity.Q.Mid
intensity.Q.High = 1 - intensity.Q.High

fitted_shape = distrParam$intensity$estimate[1]
fitted_rate  = distrParam$intensity$estimate[2]

intensLow  = qgamma(intensity.Q.Low,  shape=fitted_shape, rate=fitted_rate)
intensMid  = qgamma(intensity.Q.Mid,  shape=fitted_shape, rate=fitted_rate)
intensHigh = qgamma(intensity.Q.High, shape=fitted_shape, rate=fitted_rate)
```

```{r}
# 5. Use reconstructed quantiles as predictions of conditional quantiles of 
#    Y given X. In particular, predict Y by the 50% quantile (median)

lowMidHigh = data.frame(
  "intensLow" = intensLow,  
  "intensMid" = intensMid, 
  "intensHigh" = intensHigh
  )

df <- cbind(part2_data, lowMidHigh)
anomalies <- df[df$intensity > df$intensHigh,]

plot(part2_data$temperature,part2_data$intensity)
points(part2_data$temperature,lowMidHigh[,"intensLow"],col="orange")
points(part2_data$temperature,lowMidHigh[,"intensHigh"],col="red")
points(part2_data$temperature,lowMidHigh[,"intensMid"],col="cyan")
points(part2_data$temperature[anomalies$minute],
       part2_data$intensity[anomalies$minute],col="magenta",pch=16)

```


## Step 5

```{r}

dat <- readRDS(paste(dataPath,'step_5_data.rds',sep = '/'))
dat$predictor_DistrType
dat$output_DistrType
test_copula_data = pobs(cbind(dat$predictor, dat$output))
plot(test_copula_data)

test_imgX <- plogis(dat$predictor, location=dat$predictor_DistrParameters[1], scale=dat$predictor_DistrParameters[2])


fit.copula.clayton.test <- fitCopula(
  copula = claytonCopula(),
  data = test_copula_data,
  method = "ml",
  optim.method = "BFGS")
fit.copula.clayton.test@loglik

theta <- fit.copula.clayton.test@estimate
qclayton <- function(X, alpha_, theta_) {
  # Clayton conditional distribution quantile
  ((alpha_**(-theta_/(1+theta_)) - 1) * X**(-theta_) + 1) ** (-1/theta_)
}
test.output.Q.Low  <- qclayton(test_imgX, alpha_=0.05, theta_=theta)
test.output.Q.Mid  <- qclayton(test_imgX, alpha_=0.50, theta_=theta)
test.output.Q.High <- qclayton(test_imgX, alpha_=0.95, theta_=theta)

test_fitted_mean = dat$output_DistrParameters[1]
test_fitted_sd = dat$output_DistrParameters[2]

test.output.Low  = qlnorm(test.output.Q.Low,  meanlog=test_fitted_mean, sdlog=test_fitted_sd)
test.output.Mid  = qlnorm(test.output.Q.Mid,  meanlog=test_fitted_mean, sdlog=test_fitted_sd)
test.output.High = qlnorm(test.output.Q.High, meanlog=test_fitted_mean, sdlog=test_fitted_sd)

plot(dat$predictor,dat$output)
points(dat$predictor,test.output.Low,col="orange")
points(dat$predictor,test.output.Mid,col="red")
points(dat$predictor,test.output.High,col="cyan")


res <- list(anomalies_temperature = anomalies$temperature,
            anomalies_intensity = anomalies$intensity,
            copulaType = "clayton",
            quantileLow=test.output.Low,
            quantileMid=test.output.Mid,
            quantileHigh=test.output.High
)
saveRDS(res, file = paste(dataPath,'result5.rds',sep = '/'))
```

## Step 6


```{r}
# Step 6 - Count Regression
poission.model <- glm(intensity ~ temperature, family=poisson(link='log'), data=part2_data)

summary(poission.model)
summary(poission.model)$deviance
# 272.786, 248

nb.model <- MASS::glm.nb(intensity ~ temperature, data=part2_data)
summary(nb.model)
summary(nb.model)$deviance
# 272.761, 248, 97524

```


