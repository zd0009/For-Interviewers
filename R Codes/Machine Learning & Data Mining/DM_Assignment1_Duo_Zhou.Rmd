---
title: "MSCA 31008 Assignment 1"
author: "Duo Zhou"
date: "7/5/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---
```{r}
#Loading Data
dataPath <- "C:/Users/zd000/Desktop/MSCA/Data Mining/Assignments/week1"
GC <-read.csv(paste(dataPath, 'GermanCredit.csv', sep = '/'), header = TRUE)
GC<-GC[,-1]
```
## Use GermanCredit data [Canvas or from UCI learning site or pkg Caret R Build a regression model to predict variable "Amount" as a function of other variables (choose variables that you think are necessary and required to build a good model)

```{r}
# fit a linear regression with all variables (full model)
full.model <- lm(GC$Amount~.,data=GC)
(full.model.r.square <- summary(full.model)$r.squared)
# fit linear regression with only intercept (null model)
null.model <- lm(GC$Amount~1,data=GC)
(null.model.r.square <- summary(null.model)$r.square)
```
### Use step function to find the model with the best AIC using backwards, forwards and both Directions

```{r}
best.AIC.model.forward <- step(null.model,scope=formula(full.model),direction="forward",trace=FALSE)
best.AIC.model.backward <- step(full.model,direction="backward",trace=FALSE)
best.AIC.model.both <- step(full.model,direction="both",trace=FALSE)
```


```{r}
# Exact AIC and Number of predictor + Intercept
extractAIC(best.AIC.model.forward )
extractAIC(best.AIC.model.backward )
extractAIC(best.AIC.model.both)
```
### The forward method gives the best AIC and 18 predictors. The Other two generates 29 predictors. I picked forward method model. Since it is required that we select variables with no fewer than 20 coefficients, I added two more variables from the original data: Foreign.Work and Purpose.NewCar 

```{r}
summary(best.AIC.model.forward)
```

### Choose all the variables list above in the forward method model plus  Foreign.Work and Purpose.NewCar.
```{r}
#GC1<- GC[,c(1,2,3,8,10,11,13,15,20,21,22,23,24,25,26,28,29,31,32,33,39,43,47,49,50,51,59,60,61)]
GC2<- GC[,c(1,2,3,62,43,8,21,30,10,52,13,15,35,47,23,11,49,39,9,20)]
extractAIC(lm(Amount~.,data=GC2))
```
### We can see that with 20 variables, the model still generates less AIC than backward and both direction models.

## Split the sample randomly into training-test using a 632:368 ratio, and compute the coefficient and r-squares in training and test samples. Run the process 1000 times and save the results.
```{r}
rsquare_train <- matrix(NA,1000)
rsquare_test <- matrix(NA,1000)
coefficients <- matrix(NA,20,1000)
set.seed(1234)
for (i in 1:1000){
  train_ind <- sample(nrow(GC2), size = 0.632 * nrow(GC2))
  train <- GC2[train_ind, ]
  test <- GC2[-train_ind, ]
  fit.lm <- lm(train$Amount~.,data=train)
  coefficients[,i] <- coef(fit.lm)
  rsquare_train[i] <- summary(fit.lm)$r.squared
  predited.value <- predict(fit.lm,newdata=test,type="response")
  rsquare_test[i] <- cor(test$Amount,predited.value)^2
}

```
## Plot the distributions of all coefficients, test R^2, and % fall in R^2.

### Plot the histagram of Intercept
```{r}
rownames(coefficients)<-names(lm(Amount~.,data=GC2)$coefficients)
hist(coefficients[1,],xlab=rownames(coefficients)[1],ylab='Frequency',
     main='Distribution of Intercept')

```

### Plot the histagram of Duration
```{r}
hist(coefficients[2,],xlab=rownames(coefficients)[2],ylab='Frequency',
     main='Distribution of Duration')
```
### Plot the histagram of InstallmentRatePercentage
```{r}
rownames(coefficients)<-names(lm(Amount~.,data=GC2)$coefficients)
hist(coefficients[3,],xlab=rownames(coefficients)[3],ylab='Frequency',
     main='Distribution of InstallmentRatePercentage ')

```


### plot the distribution of Train R^2
```{r}
hist(rsquare_train,xlab="Trained R Square",ylab='Frequency')
```
###  plot the distribution of Test R^2
```{r}
hist(rsquare_test,ylab='Frequency',xlab="Test R Square")
```

### plot the distribution of % fall in R^2
```{r}
Pct.fall.r.sqrt<-(rsquare_train-rsquare_test)/rsquare_train
hist(Pct.fall.r.sqrt,
     ylab='Frequency', xlab="% Fall in R Square")
```

### Table of Train, Test and % Fall R^2
```{r}
r.square.table<-cbind(rsquare_train,rsquare_test,Pct.fall.r.sqrt)
colnames(r.square.table)<-c("Train.R.Square","Test.R.Square","%Fall.R.Square")
head(r.square.table)
```
```{r}
# Comparison of Average Train and Test R Square
mean.r.square.table<-cbind(mean(rsquare_train),mean(rsquare_test),
            (mean(rsquare_train)-mean(rsquare_test))/mean(rsquare_train))

colnames(mean.r.square.table)<-c("Avg.Train.R.Square","Avg.Test.R.Square","Avg.%Fall.R.Square")
mean.r.square.table
```

### The above graphes show that the test R Square is on average smaller than train R square. The percentage fall of R square range betweem -0.3 to 0.3. Over 80% of the runs generate %Fall of R Sqaure between -0.05 and 0.15. One average, We expect the R square to lose 5.62% from Train to Test.   

## Build linear model using entire sample.
```{r}
lm.entire <- lm(GC2$Amount~.,data=GC2)
```

## Compute the mean and standard deviation of all 1000 coefficients (for each beta)
```{r}
coef.mean <-  apply(coefficients,1,mean)
coef.sd <-  apply(coefficients,1,sd)
names(coef.mean) <- names(lm.entire$coefficients)
names(coef.sd) <- names(lm.entire$coefficients)
cbind(coef.mean=coef.mean,coef.sd=coef.sd)
```

## Compare average across 1000 to single model built using entire sample
```{r}
cbind(Averaged.Coef=coef.mean,Entire.Model.Coef=lm.entire$coefficients,
      "% Difference"=(lm.entire$coefficients-coef.mean)/lm.entire$coefficients)
```
### The coefficients from the Model fitted using entire sample is very similar to the Averaged Coefficients of the 1000 training sample runs. The difference on average is less that 1%.

## Sort each coefficient's 1000 values.
```{r}
rownames(coefficients)<- names(lm.entire$coefficients)
head.matrix(apply(coefficients, 1, sort))
```
### Compute 2.5%-97.5% Confidence Intervals (CI). Scale these CI's down by a factor of .632^0.5

```{r}
CI_lower<-rep(NA,20)
CI_upper<-rep(NA,20)
for (i in 1:20){
    CI_lower[i] <- coef.mean[i]+qnorm(0.025)*(coef.sd[i])
    CI_upper[i] <- coef.mean[i]+qnorm(0.975)*(coef.sd[i])
}
scaled.CI <- cbind("Scaled 2.5%"=CI_lower," Scaled 97.5"=CI_upper,
                   "Scaled With"=(CI_upper-CI_lower)*sqrt(0.632))
rownames(scaled.CI)<-names(lm.entire$coefficients)
```

### compute single model's CIs
```{r}
single.model.CI <- confint(lm.entire,names(lm.entire$coefficients),level=0.95)
                        
single.model.CI.width<-cbind(single.model.CI,'width'=(single.model.CI[,2]-single.model.CI[,1]))

```
## How do these Scaled CIs compare to CIs computed from single model's CIs? Tighter or broader?
```{r}
# CI compariaion table
CI.compare<-cbind(scaled.CI=scaled.CI,single.model.CI=single.model.CI.width)
CI.compare
```
## Number of CI tighter or broader.

```{r}
paste("Numer of CIs tightened: ",sum(CI.compare[,3] < CI.compare[,6]))
paste("Numer of CIs broadened: ",sum(CI.compare[,3] > CI.compare[,6]))      
```
### Standard deviation of Coefficients where CI of sampled runs tightened
```{r}
coef.sd[CI.compare[,3] < CI.compare[,6]]
```
### Standard deviation of Coefficient where CI of sampled runs broadened 
```{r}
coef.sd[CI.compare[,3] > CI.compare[,6]]
```
### According to the tables above, we find 19 variables generated smaller CIs and 1 variable generated larger CI when using repeated sample models. The Coefficents of varaibles with tighter CIs tend to vary much less (smaller variance/sd) than the coefficient with broader CI when generated by random sampling model. Hence we could conclude that repeating the model fitting process with random smaples multiple times can help improving the precision of our model coefficients. Increasing number of repeatitions will improve CIs; howerer once a critical value of repetition is acheive, the improvement of CIs becomes unnoticeable. In this assignment, the result of random sample modeling 10,000 times is very similar to the result of modeling 1000 times. 




